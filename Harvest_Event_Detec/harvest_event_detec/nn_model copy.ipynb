{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:47:49.081542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try only with best performing numeric columns for input into model first\n",
    "# TODO select features for deep learning model after ranking features with random forest (feature importance)\n",
    "# TODO lstm network\n",
    "# TODO SMOTE algorithm, for upsampling havested fields; we only have ~300 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local scripts\n",
    "from scripts import veg_indices, utilities, plots\n",
    "#from scripts.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "BANDS_DICT = {   'B2': 'Blue',\n",
    "            'B3': 'Green',\n",
    "            'B4': 'Red',\n",
    "            'B5': 'Red_Edge_1',\n",
    "            'B6': 'Red_Edge_2',\n",
    "            'B7': 'Red_Edge_3',\n",
    "            'B8': 'NIR',\n",
    "            'B8A': 'Red_Edge_4',\n",
    "            'B11': 'SWIR_1',\n",
    "            'B12': 'SWIR_2'}\n",
    "\n",
    "BANDS = list(BANDS_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "DF = geopandas.read_file('../data/merged_images.geojson')\n",
    "DF.rename(columns = {'is_within_period':'har_evnt'}, inplace = True)\n",
    "NUM_SAMPLES = len(np.unique(DF.image_idx)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>finHarvDat</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>point_idx</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>har_evnt</th>\n",
       "      <th>image_idx</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-08-11</td>\n",
       "      <td>50.655048</td>\n",
       "      <td>25.458684</td>\n",
       "      <td>p0</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "      <td>POINT (25.45868 50.65505)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>50.294633</td>\n",
       "      <td>36.289471</td>\n",
       "      <td>p1</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "      <td>POINT (36.28947 50.29463)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>50.860849</td>\n",
       "      <td>32.548484</td>\n",
       "      <td>p2</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "      <td>POINT (32.54848 50.86085)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>50.928689</td>\n",
       "      <td>31.637558</td>\n",
       "      <td>p3</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "      <td>POINT (31.63756 50.92869)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.07150</td>\n",
       "      <td>0.07760</td>\n",
       "      <td>0.08510</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>0.256335</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>48.307964</td>\n",
       "      <td>37.348245</td>\n",
       "      <td>p4</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "      <td>POINT (37.34824 48.30796)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9481</th>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.11695</td>\n",
       "      <td>0.14255</td>\n",
       "      <td>0.15525</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.327342</td>\n",
       "      <td>NaT</td>\n",
       "      <td>49.723156</td>\n",
       "      <td>36.817274</td>\n",
       "      <td>p553</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>i16</td>\n",
       "      <td>POINT (36.81727 49.72316)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>0.1616</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>0.08950</td>\n",
       "      <td>0.09310</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.373882</td>\n",
       "      <td>NaT</td>\n",
       "      <td>45.578351</td>\n",
       "      <td>28.700480</td>\n",
       "      <td>p554</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>i16</td>\n",
       "      <td>POINT (28.70048 45.57835)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9483</th>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.23980</td>\n",
       "      <td>0.26430</td>\n",
       "      <td>0.2832</td>\n",
       "      <td>0.2783</td>\n",
       "      <td>0.720535</td>\n",
       "      <td>NaT</td>\n",
       "      <td>46.693510</td>\n",
       "      <td>35.051003</td>\n",
       "      <td>p555</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>i16</td>\n",
       "      <td>POINT (35.05100 46.69351)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9484</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>50.451929</td>\n",
       "      <td>33.668204</td>\n",
       "      <td>p556</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>i16</td>\n",
       "      <td>POINT (33.66820 50.45193)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9485</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>46.994448</td>\n",
       "      <td>30.633461</td>\n",
       "      <td>p557</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>i16</td>\n",
       "      <td>POINT (30.63346 46.99445)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9486 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         B11     B12     B2      B3      B4       B5       B6       B7  \\\n",
       "0     0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "1     0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "2     0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "3     0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "4     0.1424  0.1083  0.030  0.0432  0.0631  0.07150  0.07760  0.08510   \n",
       "...      ...     ...    ...     ...     ...      ...      ...      ...   \n",
       "9481  0.2557  0.1875  0.065  0.0740  0.0973  0.11695  0.14255  0.15525   \n",
       "9482  0.1616  0.1235  0.028  0.0442  0.0525  0.06390  0.08950  0.09310   \n",
       "9483  0.1675  0.1030  0.043  0.0678  0.0460  0.10960  0.23980  0.26430   \n",
       "9484  0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "9485  0.1000  0.1000  0.100  0.1000  0.1000  0.10000  0.10000  0.10000   \n",
       "\n",
       "          B8     B8A      NDVI finHarvDat        lat        lon point_idx  \\\n",
       "0     0.1000  0.1000  0.000000 2022-08-11  50.655048  25.458684        p0   \n",
       "1     0.1000  0.1000  0.000000        NaT  50.294633  36.289471        p1   \n",
       "2     0.1000  0.1000  0.000000        NaT  50.860849  32.548484        p2   \n",
       "3     0.1000  0.1000  0.000000        NaT  50.928689  31.637558        p3   \n",
       "4     0.1066  0.1043  0.256335 2022-07-23  48.307964  37.348245        p4   \n",
       "...      ...     ...       ...        ...        ...        ...       ...   \n",
       "9481  0.1920  0.1847  0.327342        NaT  49.723156  36.817274      p553   \n",
       "9482  0.1152  0.1073  0.373882        NaT  45.578351  28.700480      p554   \n",
       "9483  0.2832  0.2783  0.720535        NaT  46.693510  35.051003      p555   \n",
       "9484  0.1000  0.1000  0.000000 2022-07-30  50.451929  33.668204      p556   \n",
       "9485  0.1000  0.1000  0.000000        NaT  46.994448  30.633461      p557   \n",
       "\n",
       "     start_date   end_date  har_evnt image_idx                   geometry  \n",
       "0    2022-01-08 2022-01-28     False        i0  POINT (25.45868 50.65505)  \n",
       "1    2022-01-08 2022-01-28     False        i0  POINT (36.28947 50.29463)  \n",
       "2    2022-01-08 2022-01-28     False        i0  POINT (32.54848 50.86085)  \n",
       "3    2022-01-08 2022-01-28     False        i0  POINT (31.63756 50.92869)  \n",
       "4    2022-01-08 2022-01-28     False        i0  POINT (37.34824 48.30796)  \n",
       "...         ...        ...       ...       ...                        ...  \n",
       "9481 2022-12-10 2022-12-30     False       i16  POINT (36.81727 49.72316)  \n",
       "9482 2022-12-10 2022-12-30     False       i16  POINT (28.70048 45.57835)  \n",
       "9483 2022-12-10 2022-12-30     False       i16  POINT (35.05100 46.69351)  \n",
       "9484 2022-12-10 2022-12-30     False       i16  POINT (33.66820 50.45193)  \n",
       "9485 2022-12-10 2022-12-30     False       i16  POINT (30.63346 46.99445)  \n",
       "\n",
       "[9486 rows x 20 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added:  ['DVI', 'RVI', 'ARVI', 'PSSRa', 'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv', 'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI']\n",
      "Added:  ['sample_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff', 'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff', 'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff', 'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff', 'NDWI_diff', 'NDSI_diff', 'NDVI_diff']\n",
      "Index(['B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'NDVI',\n",
      "       'finHarvDat', 'lat', 'lon', 'point_idx', 'start_date', 'end_date',\n",
      "       'har_evnt', 'image_idx', 'geometry', 'DVI', 'RVI', 'ARVI', 'PSSRa',\n",
      "       'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv',\n",
      "       'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI',\n",
      "       'pt_idx', 'img_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff',\n",
      "       'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff',\n",
      "       'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff',\n",
      "       'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff',\n",
      "       'NDWI_diff', 'NDSI_diff', 'NDVI_diff', 'sample_idx'],\n",
      "      dtype='object') (6678, 66)\n"
     ]
    }
   ],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "df = DF.copy()\n",
    "df = df[(df.NDVI) != 0] # drop invalid points\n",
    "VEG_INDICES_NAMES = veg_indices.add_veg_indices(df) + ['NDVI'] \n",
    "VEG_DIFF_NAMES = veg_indices.add_veg_diff(df, VEG_INDICES_NAMES)\n",
    "NUMERIC_COLS = BANDS + VEG_INDICES_NAMES + VEG_DIFF_NAMES\n",
    "df = df.dropna(subset=['sample_idx']) # removes the first image, because sample_idx is NaN there\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "# only look at samples 6, 7, 8, 9\n",
    "new_df = None\n",
    "for i in range(6, 9 + 1):\n",
    "\n",
    "    curr_df = df[df[\"sample_idx\"] == f\"s{i}\"]\n",
    "    if(type(new_df) == type(None)):\n",
    "        new_df = curr_df\n",
    "    else:\n",
    "        new_df = pd.concat([new_df, curr_df])\n",
    "df = new_df\n",
    "\"\"\"\n",
    "\n",
    "print(df.columns, df.shape)\n",
    "\n",
    "\n",
    "# For each 3-week image, standarize each column\n",
    "df = utilities.get_rm_outlier_standarize(df, NUMERIC_COLS, rm_outliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>...</th>\n",
       "      <th>NDTI_diff</th>\n",
       "      <th>NDMI_diff</th>\n",
       "      <th>MSI_diff</th>\n",
       "      <th>GCI_diff</th>\n",
       "      <th>NBRI_diff</th>\n",
       "      <th>BSI_diff</th>\n",
       "      <th>NDWI_diff</th>\n",
       "      <th>NDSI_diff</th>\n",
       "      <th>NDVI_diff</th>\n",
       "      <th>sample_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>-1.733345</td>\n",
       "      <td>-1.218163</td>\n",
       "      <td>2.538801</td>\n",
       "      <td>2.565978</td>\n",
       "      <td>2.582734</td>\n",
       "      <td>2.652148</td>\n",
       "      <td>2.583266</td>\n",
       "      <td>2.535112</td>\n",
       "      <td>2.496266</td>\n",
       "      <td>2.521000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>2.354683</td>\n",
       "      <td>-1.860549</td>\n",
       "      <td>-0.290191</td>\n",
       "      <td>1.705637</td>\n",
       "      <td>-1.810909</td>\n",
       "      <td>2.025189</td>\n",
       "      <td>2.149293</td>\n",
       "      <td>-2.144416</td>\n",
       "      <td>s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.030531</td>\n",
       "      <td>-0.465189</td>\n",
       "      <td>-0.464070</td>\n",
       "      <td>-0.425066</td>\n",
       "      <td>-0.491252</td>\n",
       "      <td>-0.582446</td>\n",
       "      <td>-0.605294</td>\n",
       "      <td>-0.581443</td>\n",
       "      <td>-0.624426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310313</td>\n",
       "      <td>-0.636401</td>\n",
       "      <td>0.591765</td>\n",
       "      <td>-0.564848</td>\n",
       "      <td>0.072627</td>\n",
       "      <td>0.642642</td>\n",
       "      <td>-0.459362</td>\n",
       "      <td>-0.630151</td>\n",
       "      <td>0.216412</td>\n",
       "      <td>s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>-0.072429</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>-0.500069</td>\n",
       "      <td>-0.508641</td>\n",
       "      <td>-0.495575</td>\n",
       "      <td>-0.544710</td>\n",
       "      <td>-0.658926</td>\n",
       "      <td>-0.675985</td>\n",
       "      <td>-0.673982</td>\n",
       "      <td>-0.693985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090693</td>\n",
       "      <td>-0.705486</td>\n",
       "      <td>0.964065</td>\n",
       "      <td>-0.757482</td>\n",
       "      <td>-0.094059</td>\n",
       "      <td>0.974112</td>\n",
       "      <td>-0.657753</td>\n",
       "      <td>-0.786087</td>\n",
       "      <td>0.373323</td>\n",
       "      <td>s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-0.475597</td>\n",
       "      <td>-0.137859</td>\n",
       "      <td>2.456404</td>\n",
       "      <td>2.440304</td>\n",
       "      <td>2.439523</td>\n",
       "      <td>2.404264</td>\n",
       "      <td>2.361241</td>\n",
       "      <td>2.361878</td>\n",
       "      <td>2.338312</td>\n",
       "      <td>2.323263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554492</td>\n",
       "      <td>1.747677</td>\n",
       "      <td>-1.632557</td>\n",
       "      <td>-0.370456</td>\n",
       "      <td>1.624033</td>\n",
       "      <td>-1.573719</td>\n",
       "      <td>1.942476</td>\n",
       "      <td>1.756096</td>\n",
       "      <td>-1.934649</td>\n",
       "      <td>s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.883804</td>\n",
       "      <td>0.933417</td>\n",
       "      <td>-0.442766</td>\n",
       "      <td>-0.456764</td>\n",
       "      <td>-0.437304</td>\n",
       "      <td>-0.435597</td>\n",
       "      <td>-0.451143</td>\n",
       "      <td>-0.456309</td>\n",
       "      <td>-0.456197</td>\n",
       "      <td>-0.459937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159611</td>\n",
       "      <td>-0.547000</td>\n",
       "      <td>0.575539</td>\n",
       "      <td>-0.129763</td>\n",
       "      <td>-0.353626</td>\n",
       "      <td>0.643998</td>\n",
       "      <td>-0.627675</td>\n",
       "      <td>-0.578638</td>\n",
       "      <td>0.521988</td>\n",
       "      <td>s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9475</th>\n",
       "      <td>3.322104</td>\n",
       "      <td>2.328019</td>\n",
       "      <td>0.132010</td>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.095625</td>\n",
       "      <td>0.472431</td>\n",
       "      <td>1.725177</td>\n",
       "      <td>2.015223</td>\n",
       "      <td>2.066392</td>\n",
       "      <td>2.180961</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.826355</td>\n",
       "      <td>0.732779</td>\n",
       "      <td>-1.169280</td>\n",
       "      <td>1.612652</td>\n",
       "      <td>-0.915200</td>\n",
       "      <td>-1.344209</td>\n",
       "      <td>0.282752</td>\n",
       "      <td>0.795201</td>\n",
       "      <td>0.566191</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9478</th>\n",
       "      <td>-1.628924</td>\n",
       "      <td>-1.062433</td>\n",
       "      <td>3.578395</td>\n",
       "      <td>3.457746</td>\n",
       "      <td>3.404489</td>\n",
       "      <td>3.259756</td>\n",
       "      <td>3.083942</td>\n",
       "      <td>2.963168</td>\n",
       "      <td>3.031307</td>\n",
       "      <td>2.897218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668641</td>\n",
       "      <td>2.848363</td>\n",
       "      <td>-2.258848</td>\n",
       "      <td>-0.110247</td>\n",
       "      <td>1.915134</td>\n",
       "      <td>-2.202332</td>\n",
       "      <td>2.572595</td>\n",
       "      <td>2.716314</td>\n",
       "      <td>-2.818478</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9481</th>\n",
       "      <td>1.843392</td>\n",
       "      <td>1.368079</td>\n",
       "      <td>-0.275171</td>\n",
       "      <td>-0.257938</td>\n",
       "      <td>-0.181384</td>\n",
       "      <td>-0.168046</td>\n",
       "      <td>-0.179112</td>\n",
       "      <td>-0.158634</td>\n",
       "      <td>-0.057645</td>\n",
       "      <td>-0.060312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114312</td>\n",
       "      <td>-0.434865</td>\n",
       "      <td>0.208224</td>\n",
       "      <td>0.134505</td>\n",
       "      <td>-0.290227</td>\n",
       "      <td>0.266190</td>\n",
       "      <td>-0.606048</td>\n",
       "      <td>-0.316902</td>\n",
       "      <td>0.445377</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>-0.009429</td>\n",
       "      <td>-0.141409</td>\n",
       "      <td>-0.468817</td>\n",
       "      <td>-0.419537</td>\n",
       "      <td>-0.416868</td>\n",
       "      <td>-0.447121</td>\n",
       "      <td>-0.460629</td>\n",
       "      <td>-0.493758</td>\n",
       "      <td>-0.450153</td>\n",
       "      <td>-0.492917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411511</td>\n",
       "      <td>-0.449486</td>\n",
       "      <td>0.316610</td>\n",
       "      <td>0.088229</td>\n",
       "      <td>-0.345191</td>\n",
       "      <td>0.438725</td>\n",
       "      <td>-0.761268</td>\n",
       "      <td>-0.373365</td>\n",
       "      <td>0.573503</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9483</th>\n",
       "      <td>0.106742</td>\n",
       "      <td>-0.624917</td>\n",
       "      <td>-0.390312</td>\n",
       "      <td>-0.291560</td>\n",
       "      <td>-0.451034</td>\n",
       "      <td>-0.206712</td>\n",
       "      <td>0.336959</td>\n",
       "      <td>0.429383</td>\n",
       "      <td>0.408457</td>\n",
       "      <td>0.462838</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.675552</td>\n",
       "      <td>0.465889</td>\n",
       "      <td>-1.073355</td>\n",
       "      <td>1.948092</td>\n",
       "      <td>-1.352097</td>\n",
       "      <td>-1.349143</td>\n",
       "      <td>0.104137</td>\n",
       "      <td>0.640749</td>\n",
       "      <td>0.962237</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6678 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B11       B12        B2        B3        B4        B5        B6  \\\n",
       "560  -1.733345 -1.218163  2.538801  2.565978  2.582734  2.652148  2.583266   \n",
       "562   0.091251 -0.030531 -0.465189 -0.464070 -0.425066 -0.491252 -0.582446   \n",
       "563  -0.072429  0.107892 -0.500069 -0.508641 -0.495575 -0.544710 -0.658926   \n",
       "564  -0.475597 -0.137859  2.456404  2.440304  2.439523  2.404264  2.361241   \n",
       "565   0.883804  0.933417 -0.442766 -0.456764 -0.437304 -0.435597 -0.451143   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9475  3.322104  2.328019  0.132010  0.231196  0.095625  0.472431  1.725177   \n",
       "9478 -1.628924 -1.062433  3.578395  3.457746  3.404489  3.259756  3.083942   \n",
       "9481  1.843392  1.368079 -0.275171 -0.257938 -0.181384 -0.168046 -0.179112   \n",
       "9482 -0.009429 -0.141409 -0.468817 -0.419537 -0.416868 -0.447121 -0.460629   \n",
       "9483  0.106742 -0.624917 -0.390312 -0.291560 -0.451034 -0.206712  0.336959   \n",
       "\n",
       "            B7        B8       B8A  ...  NDTI_diff NDMI_diff  MSI_diff  \\\n",
       "560   2.535112  2.496266  2.521000  ...   0.000192  2.354683 -1.860549   \n",
       "562  -0.605294 -0.581443 -0.624426  ...   0.310313 -0.636401  0.591765   \n",
       "563  -0.675985 -0.673982 -0.693985  ...   0.090693 -0.705486  0.964065   \n",
       "564   2.361878  2.338312  2.323263  ...   0.554492  1.747677 -1.632557   \n",
       "565  -0.456309 -0.456197 -0.459937  ...  -0.159611 -0.547000  0.575539   \n",
       "...        ...       ...       ...  ...        ...       ...       ...   \n",
       "9475  2.015223  2.066392  2.180961  ...  -4.826355  0.732779 -1.169280   \n",
       "9478  2.963168  3.031307  2.897218  ...  -0.668641  2.848363 -2.258848   \n",
       "9481 -0.158634 -0.057645 -0.060312  ...   0.114312 -0.434865  0.208224   \n",
       "9482 -0.493758 -0.450153 -0.492917  ...   0.411511 -0.449486  0.316610   \n",
       "9483  0.429383  0.408457  0.462838  ...  -1.675552  0.465889 -1.073355   \n",
       "\n",
       "      GCI_diff NBRI_diff  BSI_diff NDWI_diff  NDSI_diff NDVI_diff sample_idx  \n",
       "560  -0.290191  1.705637 -1.810909  2.025189   2.149293 -2.144416         s0  \n",
       "562  -0.564848  0.072627  0.642642 -0.459362  -0.630151  0.216412         s0  \n",
       "563  -0.757482 -0.094059  0.974112 -0.657753  -0.786087  0.373323         s0  \n",
       "564  -0.370456  1.624033 -1.573719  1.942476   1.756096 -1.934649         s0  \n",
       "565  -0.129763 -0.353626  0.643998 -0.627675  -0.578638  0.521988         s0  \n",
       "...        ...       ...       ...       ...        ...       ...        ...  \n",
       "9475  1.612652 -0.915200 -1.344209  0.282752   0.795201  0.566191        s15  \n",
       "9478 -0.110247  1.915134 -2.202332  2.572595   2.716314 -2.818478        s15  \n",
       "9481  0.134505 -0.290227  0.266190 -0.606048  -0.316902  0.445377        s15  \n",
       "9482  0.088229 -0.345191  0.438725 -0.761268  -0.373365  0.573503        s15  \n",
       "9483  1.948092 -1.352097 -1.349143  0.104137   0.640749  0.962237        s15  \n",
       "\n",
       "[6678 rows x 66 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(df:pd.DataFrame, colName: str, retArrays=False):\n",
    "    lookupTable, indexed_dataSet = np.unique(df[colName], return_inverse=True)\n",
    "    tpls = tf.keras.utils.to_categorical(indexed_dataSet)\n",
    "\n",
    "    arrays = tpls.T[1:] # remove one column (not needed)\n",
    "    arrays = arrays.astype(np.float32) # tensorflow does not support bools, so float 32 is the best we can do :/\n",
    "    if(retArrays):\n",
    "        return arrays\n",
    "\n",
    "    return arrays.T # return to n by m (n tuples of size m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>...</th>\n",
       "      <th>NDTI_diff</th>\n",
       "      <th>NDMI_diff</th>\n",
       "      <th>MSI_diff</th>\n",
       "      <th>GCI_diff</th>\n",
       "      <th>NBRI_diff</th>\n",
       "      <th>BSI_diff</th>\n",
       "      <th>NDWI_diff</th>\n",
       "      <th>NDSI_diff</th>\n",
       "      <th>NDVI_diff</th>\n",
       "      <th>sample_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>-0.870086</td>\n",
       "      <td>-0.894463</td>\n",
       "      <td>-0.362898</td>\n",
       "      <td>-0.562899</td>\n",
       "      <td>-0.712486</td>\n",
       "      <td>-0.803480</td>\n",
       "      <td>-0.029281</td>\n",
       "      <td>0.751010</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.729749</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.478007</td>\n",
       "      <td>1.603634</td>\n",
       "      <td>-1.360793</td>\n",
       "      <td>2.700617</td>\n",
       "      <td>-2.968218</td>\n",
       "      <td>-1.523347</td>\n",
       "      <td>0.521851</td>\n",
       "      <td>1.005915</td>\n",
       "      <td>1.573112</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>-0.312600</td>\n",
       "      <td>-0.369832</td>\n",
       "      <td>-0.086144</td>\n",
       "      <td>0.280409</td>\n",
       "      <td>-0.099855</td>\n",
       "      <td>0.208097</td>\n",
       "      <td>0.629703</td>\n",
       "      <td>0.362093</td>\n",
       "      <td>0.061773</td>\n",
       "      <td>0.152029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.370216</td>\n",
       "      <td>-0.424915</td>\n",
       "      <td>-0.590995</td>\n",
       "      <td>0.906889</td>\n",
       "      <td>-0.333211</td>\n",
       "      <td>1.112289</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>-0.610115</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>0.709577</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-0.152589</td>\n",
       "      <td>-0.109988</td>\n",
       "      <td>-0.493141</td>\n",
       "      <td>-0.196534</td>\n",
       "      <td>1.214141</td>\n",
       "      <td>1.359916</td>\n",
       "      <td>1.234850</td>\n",
       "      <td>1.216424</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.587707</td>\n",
       "      <td>-0.081406</td>\n",
       "      <td>-0.394088</td>\n",
       "      <td>0.704432</td>\n",
       "      <td>-0.719594</td>\n",
       "      <td>-0.579386</td>\n",
       "      <td>-0.135283</td>\n",
       "      <td>0.200484</td>\n",
       "      <td>1.265219</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>-0.561720</td>\n",
       "      <td>-0.722576</td>\n",
       "      <td>-0.312302</td>\n",
       "      <td>-0.473593</td>\n",
       "      <td>-0.729585</td>\n",
       "      <td>-0.671262</td>\n",
       "      <td>0.668467</td>\n",
       "      <td>1.463120</td>\n",
       "      <td>2.055841</td>\n",
       "      <td>1.519354</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.537627</td>\n",
       "      <td>1.919438</td>\n",
       "      <td>-1.531427</td>\n",
       "      <td>3.167865</td>\n",
       "      <td>-3.512676</td>\n",
       "      <td>-1.735957</td>\n",
       "      <td>0.806501</td>\n",
       "      <td>1.165113</td>\n",
       "      <td>1.692973</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>-0.299037</td>\n",
       "      <td>-0.194955</td>\n",
       "      <td>-0.324494</td>\n",
       "      <td>-0.356218</td>\n",
       "      <td>-0.293845</td>\n",
       "      <td>-0.373203</td>\n",
       "      <td>-0.754462</td>\n",
       "      <td>-0.892654</td>\n",
       "      <td>-0.904964</td>\n",
       "      <td>-0.916228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557096</td>\n",
       "      <td>-0.683240</td>\n",
       "      <td>0.500220</td>\n",
       "      <td>-0.312166</td>\n",
       "      <td>0.231992</td>\n",
       "      <td>0.551830</td>\n",
       "      <td>-0.896480</td>\n",
       "      <td>-0.580907</td>\n",
       "      <td>0.203664</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>-1.223664</td>\n",
       "      <td>-1.653289</td>\n",
       "      <td>-0.874174</td>\n",
       "      <td>-0.848826</td>\n",
       "      <td>-1.376933</td>\n",
       "      <td>-0.955374</td>\n",
       "      <td>1.352084</td>\n",
       "      <td>1.767256</td>\n",
       "      <td>1.749997</td>\n",
       "      <td>1.694362</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.565219</td>\n",
       "      <td>1.862842</td>\n",
       "      <td>-1.964314</td>\n",
       "      <td>2.941633</td>\n",
       "      <td>-3.306606</td>\n",
       "      <td>-2.374764</td>\n",
       "      <td>0.411842</td>\n",
       "      <td>2.136983</td>\n",
       "      <td>2.313166</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>0.585052</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>-0.201453</td>\n",
       "      <td>-0.167603</td>\n",
       "      <td>0.086507</td>\n",
       "      <td>-0.184328</td>\n",
       "      <td>-0.739681</td>\n",
       "      <td>-0.749316</td>\n",
       "      <td>-0.777399</td>\n",
       "      <td>-0.679339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142589</td>\n",
       "      <td>-0.838315</td>\n",
       "      <td>1.168225</td>\n",
       "      <td>-0.907196</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>1.137787</td>\n",
       "      <td>-0.727334</td>\n",
       "      <td>-1.122512</td>\n",
       "      <td>-0.399597</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135</th>\n",
       "      <td>-0.186510</td>\n",
       "      <td>0.046045</td>\n",
       "      <td>-0.259642</td>\n",
       "      <td>-0.345125</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>-0.173940</td>\n",
       "      <td>-0.687242</td>\n",
       "      <td>-0.689586</td>\n",
       "      <td>-0.657617</td>\n",
       "      <td>-0.606040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554367</td>\n",
       "      <td>-0.367829</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>-0.382788</td>\n",
       "      <td>0.256747</td>\n",
       "      <td>0.560347</td>\n",
       "      <td>-0.615972</td>\n",
       "      <td>-0.461928</td>\n",
       "      <td>-0.459774</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6136</th>\n",
       "      <td>0.421999</td>\n",
       "      <td>1.102268</td>\n",
       "      <td>-0.817404</td>\n",
       "      <td>-1.045111</td>\n",
       "      <td>-0.845186</td>\n",
       "      <td>-1.091576</td>\n",
       "      <td>-1.444705</td>\n",
       "      <td>-1.206955</td>\n",
       "      <td>-1.128188</td>\n",
       "      <td>-1.259767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706420</td>\n",
       "      <td>-1.028375</td>\n",
       "      <td>1.619866</td>\n",
       "      <td>-0.555706</td>\n",
       "      <td>-0.749974</td>\n",
       "      <td>1.448490</td>\n",
       "      <td>-2.587140</td>\n",
       "      <td>-1.076003</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>-0.608432</td>\n",
       "      <td>-0.292357</td>\n",
       "      <td>-0.669803</td>\n",
       "      <td>-0.638109</td>\n",
       "      <td>-0.802010</td>\n",
       "      <td>-0.760304</td>\n",
       "      <td>-0.322105</td>\n",
       "      <td>-0.291678</td>\n",
       "      <td>-0.341051</td>\n",
       "      <td>-0.349493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766941</td>\n",
       "      <td>0.605088</td>\n",
       "      <td>-0.278171</td>\n",
       "      <td>0.380161</td>\n",
       "      <td>-0.491515</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.655980</td>\n",
       "      <td>0.251211</td>\n",
       "      <td>0.849334</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2083 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B11       B12        B2        B3        B4        B5        B6  \\\n",
       "3906 -0.870086 -0.894463 -0.362898 -0.562899 -0.712486 -0.803480 -0.029281   \n",
       "3907 -0.312600 -0.369832 -0.086144  0.280409 -0.099855  0.208097  0.629703   \n",
       "3908  0.709577  0.053909 -0.152589 -0.109988 -0.493141 -0.196534  1.214141   \n",
       "3909 -0.561720 -0.722576 -0.312302 -0.473593 -0.729585 -0.671262  0.668467   \n",
       "3910 -0.299037 -0.194955 -0.324494 -0.356218 -0.293845 -0.373203 -0.754462   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6133 -1.223664 -1.653289 -0.874174 -0.848826 -1.376933 -0.955374  1.352084   \n",
       "6134  0.585052  0.995327 -0.201453 -0.167603  0.086507 -0.184328 -0.739681   \n",
       "6135 -0.186510  0.046045 -0.259642 -0.345125  0.100141 -0.173940 -0.687242   \n",
       "6136  0.421999  1.102268 -0.817404 -1.045111 -0.845186 -1.091576 -1.444705   \n",
       "6137 -0.608432 -0.292357 -0.669803 -0.638109 -0.802010 -0.760304 -0.322105   \n",
       "\n",
       "            B7        B8       B8A  ...  NDTI_diff NDMI_diff  MSI_diff  \\\n",
       "3906  0.751010  0.946631  0.729749  ...  -1.478007  1.603634 -1.360793   \n",
       "3907  0.362093  0.061773  0.152029  ...   0.003610  0.370216 -0.424915   \n",
       "3908  1.359916  1.234850  1.216424  ...  -1.587707 -0.081406 -0.394088   \n",
       "3909  1.463120  2.055841  1.519354  ...  -2.537627  1.919438 -1.531427   \n",
       "3910 -0.892654 -0.904964 -0.916228  ...   0.557096 -0.683240  0.500220   \n",
       "...        ...       ...       ...  ...        ...       ...       ...   \n",
       "6133  1.767256  1.749997  1.694362  ...  -1.565219  1.862842 -1.964314   \n",
       "6134 -0.749316 -0.777399 -0.679339  ...   0.142589 -0.838315  1.168225   \n",
       "6135 -0.689586 -0.657617 -0.606040  ...   0.554367 -0.367829  0.398082   \n",
       "6136 -1.206955 -1.128188 -1.259767  ...  -0.706420 -1.028375  1.619866   \n",
       "6137 -0.291678 -0.341051 -0.349493  ...  -0.766941  0.605088 -0.278171   \n",
       "\n",
       "      GCI_diff NBRI_diff  BSI_diff NDWI_diff  NDSI_diff NDVI_diff sample_idx  \n",
       "3906  2.700617 -2.968218 -1.523347  0.521851   1.005915  1.573112         s6  \n",
       "3907 -0.590995  0.906889 -0.333211  1.112289   0.629439 -0.610115         s6  \n",
       "3908  0.704432 -0.719594 -0.579386 -0.135283   0.200484  1.265219         s6  \n",
       "3909  3.167865 -3.512676 -1.735957  0.806501   1.165113  1.692973         s6  \n",
       "3910 -0.312166  0.231992  0.551830 -0.896480  -0.580907  0.203664         s6  \n",
       "...        ...       ...       ...       ...        ...       ...        ...  \n",
       "6133  2.941633 -3.306606 -2.374764  0.411842   2.136983  2.313166         s9  \n",
       "6134 -0.907196  0.397156  1.137787 -0.727334  -1.122512 -0.399597         s9  \n",
       "6135 -0.382788  0.256747  0.560347 -0.615972  -0.461928 -0.459774         s9  \n",
       "6136 -0.555706 -0.749974  1.448490 -2.587140  -1.076003  0.980113         s9  \n",
       "6137  0.380161 -0.491515 -0.017271 -0.655980   0.251211  0.849334         s9  \n",
       "\n",
       "[2083 rows x 66 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'NDVI',\n",
      "       'finHarvDat', 'lat', 'lon', 'point_idx', 'start_date', 'end_date',\n",
      "       'har_evnt', 'image_idx', 'geometry', 'DVI', 'RVI', 'ARVI', 'PSSRa',\n",
      "       'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv',\n",
      "       'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI',\n",
      "       'pt_idx', 'img_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff',\n",
      "       'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff',\n",
      "       'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff',\n",
      "       'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff',\n",
      "       'NDWI_diff', 'NDSI_diff', 'NDVI_diff', 'sample_idx',\n",
      "       'sample_idx_1hot_0', 'sample_idx_1hot_1', 'sample_idx_1hot_2',\n",
      "       'har_evnt_1hot_0'],\n",
      "      dtype='object')\n",
      "['sample_idx_1hot_0', 'sample_idx_1hot_1', 'sample_idx_1hot_2']\n",
      "['har_evnt_1hot_0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_to_1hot(categorical_col_name:str):\n",
    "    names = []\n",
    "    arrays = get_one_hot(df, categorical_col_name, retArrays=True)\n",
    "    for i in range(len(arrays)):\n",
    "        arr = arrays[i]\n",
    "        name = f'{categorical_col_name}_1hot_{i}'\n",
    "        df[name] = arr\n",
    "        names.append(name)\n",
    "    return names\n",
    "\n",
    "one_hot_names_X = convert_to_1hot(\"sample_idx\")\n",
    "one_hot_names_Y = convert_to_1hot(\"har_evnt\")\n",
    "\n",
    "print(df.columns)\n",
    "print(one_hot_names_X)\n",
    "print(one_hot_names_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p0',\n",
       " 'p1',\n",
       " 'p2',\n",
       " 'p3',\n",
       " 'p4',\n",
       " 'p5',\n",
       " 'p6',\n",
       " 'p7',\n",
       " 'p8',\n",
       " 'p9',\n",
       " 'p10',\n",
       " 'p11',\n",
       " 'p12',\n",
       " 'p13',\n",
       " 'p14',\n",
       " 'p15',\n",
       " 'p16',\n",
       " 'p17',\n",
       " 'p18',\n",
       " 'p19',\n",
       " 'p20',\n",
       " 'p21',\n",
       " 'p22',\n",
       " 'p23',\n",
       " 'p24',\n",
       " 'p25',\n",
       " 'p26',\n",
       " 'p27',\n",
       " 'p28',\n",
       " 'p29',\n",
       " 'p30',\n",
       " 'p31',\n",
       " 'p32',\n",
       " 'p33',\n",
       " 'p34',\n",
       " 'p35',\n",
       " 'p36',\n",
       " 'p37',\n",
       " 'p38',\n",
       " 'p39',\n",
       " 'p40',\n",
       " 'p41',\n",
       " 'p42',\n",
       " 'p43',\n",
       " 'p44',\n",
       " 'p45',\n",
       " 'p46',\n",
       " 'p47',\n",
       " 'p48',\n",
       " 'p49',\n",
       " 'p50',\n",
       " 'p51',\n",
       " 'p52',\n",
       " 'p53',\n",
       " 'p54',\n",
       " 'p55',\n",
       " 'p56',\n",
       " 'p57',\n",
       " 'p58',\n",
       " 'p59',\n",
       " 'p60',\n",
       " 'p61',\n",
       " 'p62',\n",
       " 'p63',\n",
       " 'p64',\n",
       " 'p65',\n",
       " 'p66',\n",
       " 'p67',\n",
       " 'p68',\n",
       " 'p69',\n",
       " 'p70',\n",
       " 'p71',\n",
       " 'p72',\n",
       " 'p73',\n",
       " 'p74',\n",
       " 'p75',\n",
       " 'p76',\n",
       " 'p77',\n",
       " 'p78',\n",
       " 'p79',\n",
       " 'p80',\n",
       " 'p81',\n",
       " 'p82',\n",
       " 'p83',\n",
       " 'p84',\n",
       " 'p85',\n",
       " 'p86',\n",
       " 'p87',\n",
       " 'p88',\n",
       " 'p89',\n",
       " 'p90',\n",
       " 'p91',\n",
       " 'p92',\n",
       " 'p93',\n",
       " 'p94',\n",
       " 'p95',\n",
       " 'p96',\n",
       " 'p97',\n",
       " 'p98',\n",
       " 'p99',\n",
       " 'p100',\n",
       " 'p101',\n",
       " 'p102',\n",
       " 'p103',\n",
       " 'p104',\n",
       " 'p105',\n",
       " 'p106',\n",
       " 'p107',\n",
       " 'p108',\n",
       " 'p109',\n",
       " 'p110',\n",
       " 'p111',\n",
       " 'p112',\n",
       " 'p113',\n",
       " 'p114',\n",
       " 'p115',\n",
       " 'p116',\n",
       " 'p117',\n",
       " 'p118',\n",
       " 'p119',\n",
       " 'p120',\n",
       " 'p121',\n",
       " 'p122',\n",
       " 'p123',\n",
       " 'p124',\n",
       " 'p125',\n",
       " 'p126',\n",
       " 'p127',\n",
       " 'p128',\n",
       " 'p129',\n",
       " 'p130',\n",
       " 'p131',\n",
       " 'p132',\n",
       " 'p133',\n",
       " 'p134',\n",
       " 'p135',\n",
       " 'p136',\n",
       " 'p137',\n",
       " 'p138',\n",
       " 'p139',\n",
       " 'p140',\n",
       " 'p141',\n",
       " 'p142',\n",
       " 'p143',\n",
       " 'p144',\n",
       " 'p145',\n",
       " 'p146',\n",
       " 'p147',\n",
       " 'p148',\n",
       " 'p149',\n",
       " 'p150',\n",
       " 'p151',\n",
       " 'p152',\n",
       " 'p153',\n",
       " 'p154',\n",
       " 'p155',\n",
       " 'p156',\n",
       " 'p157',\n",
       " 'p158',\n",
       " 'p159',\n",
       " 'p160',\n",
       " 'p161',\n",
       " 'p162',\n",
       " 'p163',\n",
       " 'p164',\n",
       " 'p165',\n",
       " 'p166',\n",
       " 'p167',\n",
       " 'p168',\n",
       " 'p169',\n",
       " 'p170',\n",
       " 'p171',\n",
       " 'p172',\n",
       " 'p173',\n",
       " 'p174',\n",
       " 'p175',\n",
       " 'p176',\n",
       " 'p177',\n",
       " 'p178',\n",
       " 'p179',\n",
       " 'p180',\n",
       " 'p181',\n",
       " 'p182',\n",
       " 'p183',\n",
       " 'p184',\n",
       " 'p185',\n",
       " 'p186',\n",
       " 'p187',\n",
       " 'p188',\n",
       " 'p189',\n",
       " 'p190',\n",
       " 'p191',\n",
       " 'p192',\n",
       " 'p193',\n",
       " 'p194',\n",
       " 'p195',\n",
       " 'p196',\n",
       " 'p197',\n",
       " 'p198',\n",
       " 'p199',\n",
       " 'p200',\n",
       " 'p201',\n",
       " 'p202',\n",
       " 'p203',\n",
       " 'p204',\n",
       " 'p205',\n",
       " 'p206',\n",
       " 'p207',\n",
       " 'p208',\n",
       " 'p209',\n",
       " 'p210',\n",
       " 'p211',\n",
       " 'p212',\n",
       " 'p213',\n",
       " 'p214',\n",
       " 'p215',\n",
       " 'p216',\n",
       " 'p217',\n",
       " 'p218',\n",
       " 'p219',\n",
       " 'p220',\n",
       " 'p221',\n",
       " 'p222',\n",
       " 'p223',\n",
       " 'p224',\n",
       " 'p225',\n",
       " 'p226',\n",
       " 'p227',\n",
       " 'p228',\n",
       " 'p229',\n",
       " 'p230',\n",
       " 'p231',\n",
       " 'p232',\n",
       " 'p233',\n",
       " 'p234',\n",
       " 'p235',\n",
       " 'p236',\n",
       " 'p237',\n",
       " 'p238',\n",
       " 'p239',\n",
       " 'p241',\n",
       " 'p242',\n",
       " 'p243',\n",
       " 'p244',\n",
       " 'p245',\n",
       " 'p246',\n",
       " 'p247',\n",
       " 'p248',\n",
       " 'p249',\n",
       " 'p250',\n",
       " 'p251',\n",
       " 'p252',\n",
       " 'p253',\n",
       " 'p254',\n",
       " 'p255',\n",
       " 'p256',\n",
       " 'p257',\n",
       " 'p258',\n",
       " 'p259',\n",
       " 'p260',\n",
       " 'p261',\n",
       " 'p262',\n",
       " 'p263',\n",
       " 'p264',\n",
       " 'p265',\n",
       " 'p266',\n",
       " 'p267',\n",
       " 'p268',\n",
       " 'p269',\n",
       " 'p270',\n",
       " 'p271',\n",
       " 'p272',\n",
       " 'p273',\n",
       " 'p274',\n",
       " 'p275',\n",
       " 'p276',\n",
       " 'p277',\n",
       " 'p278',\n",
       " 'p279',\n",
       " 'p280',\n",
       " 'p281',\n",
       " 'p282',\n",
       " 'p283',\n",
       " 'p284',\n",
       " 'p285',\n",
       " 'p286',\n",
       " 'p287',\n",
       " 'p288',\n",
       " 'p289',\n",
       " 'p290',\n",
       " 'p291',\n",
       " 'p292',\n",
       " 'p293',\n",
       " 'p294',\n",
       " 'p295',\n",
       " 'p296',\n",
       " 'p297',\n",
       " 'p298',\n",
       " 'p299',\n",
       " 'p300',\n",
       " 'p301',\n",
       " 'p302',\n",
       " 'p303',\n",
       " 'p304',\n",
       " 'p305',\n",
       " 'p306',\n",
       " 'p307',\n",
       " 'p308',\n",
       " 'p309',\n",
       " 'p310',\n",
       " 'p311',\n",
       " 'p312',\n",
       " 'p313',\n",
       " 'p314',\n",
       " 'p315',\n",
       " 'p316',\n",
       " 'p317',\n",
       " 'p318',\n",
       " 'p319',\n",
       " 'p320',\n",
       " 'p321',\n",
       " 'p322',\n",
       " 'p323',\n",
       " 'p324',\n",
       " 'p325',\n",
       " 'p326',\n",
       " 'p327',\n",
       " 'p328',\n",
       " 'p329',\n",
       " 'p330',\n",
       " 'p331',\n",
       " 'p332',\n",
       " 'p333',\n",
       " 'p334',\n",
       " 'p335',\n",
       " 'p336',\n",
       " 'p337',\n",
       " 'p338',\n",
       " 'p339',\n",
       " 'p340',\n",
       " 'p341',\n",
       " 'p342',\n",
       " 'p343',\n",
       " 'p344',\n",
       " 'p345',\n",
       " 'p346',\n",
       " 'p347',\n",
       " 'p348',\n",
       " 'p349',\n",
       " 'p350',\n",
       " 'p351',\n",
       " 'p352',\n",
       " 'p353',\n",
       " 'p354',\n",
       " 'p355',\n",
       " 'p356',\n",
       " 'p357',\n",
       " 'p358',\n",
       " 'p359',\n",
       " 'p360',\n",
       " 'p361',\n",
       " 'p362',\n",
       " 'p363',\n",
       " 'p364',\n",
       " 'p365',\n",
       " 'p366',\n",
       " 'p367',\n",
       " 'p368',\n",
       " 'p369',\n",
       " 'p370',\n",
       " 'p371',\n",
       " 'p372',\n",
       " 'p373',\n",
       " 'p374',\n",
       " 'p375',\n",
       " 'p376',\n",
       " 'p377',\n",
       " 'p378',\n",
       " 'p379',\n",
       " 'p380',\n",
       " 'p381',\n",
       " 'p382',\n",
       " 'p383',\n",
       " 'p384',\n",
       " 'p385',\n",
       " 'p386',\n",
       " 'p387',\n",
       " 'p388',\n",
       " 'p389',\n",
       " 'p390',\n",
       " 'p391',\n",
       " 'p392',\n",
       " 'p393',\n",
       " 'p394',\n",
       " 'p395',\n",
       " 'p396',\n",
       " 'p397',\n",
       " 'p398',\n",
       " 'p399',\n",
       " 'p400',\n",
       " 'p401',\n",
       " 'p402',\n",
       " 'p403',\n",
       " 'p404',\n",
       " 'p405',\n",
       " 'p406',\n",
       " 'p407',\n",
       " 'p408',\n",
       " 'p409',\n",
       " 'p410',\n",
       " 'p411',\n",
       " 'p412',\n",
       " 'p413',\n",
       " 'p414',\n",
       " 'p415',\n",
       " 'p416',\n",
       " 'p417',\n",
       " 'p418',\n",
       " 'p419',\n",
       " 'p420',\n",
       " 'p421',\n",
       " 'p422',\n",
       " 'p423',\n",
       " 'p424',\n",
       " 'p425',\n",
       " 'p426',\n",
       " 'p427',\n",
       " 'p428',\n",
       " 'p429',\n",
       " 'p430',\n",
       " 'p432',\n",
       " 'p433',\n",
       " 'p434',\n",
       " 'p435',\n",
       " 'p436',\n",
       " 'p437',\n",
       " 'p438',\n",
       " 'p439',\n",
       " 'p440',\n",
       " 'p441',\n",
       " 'p442',\n",
       " 'p443',\n",
       " 'p444',\n",
       " 'p445',\n",
       " 'p446',\n",
       " 'p447',\n",
       " 'p448',\n",
       " 'p449',\n",
       " 'p450',\n",
       " 'p451',\n",
       " 'p452',\n",
       " 'p453',\n",
       " 'p454',\n",
       " 'p455',\n",
       " 'p456',\n",
       " 'p457',\n",
       " 'p458',\n",
       " 'p459',\n",
       " 'p460',\n",
       " 'p461',\n",
       " 'p462',\n",
       " 'p463',\n",
       " 'p464',\n",
       " 'p465',\n",
       " 'p466',\n",
       " 'p467',\n",
       " 'p468',\n",
       " 'p469',\n",
       " 'p470',\n",
       " 'p471',\n",
       " 'p472',\n",
       " 'p473',\n",
       " 'p474',\n",
       " 'p475',\n",
       " 'p476',\n",
       " 'p477',\n",
       " 'p478',\n",
       " 'p479',\n",
       " 'p480',\n",
       " 'p481',\n",
       " 'p482',\n",
       " 'p483',\n",
       " 'p484',\n",
       " 'p485',\n",
       " 'p486',\n",
       " 'p487',\n",
       " 'p488',\n",
       " 'p489',\n",
       " 'p490',\n",
       " 'p491',\n",
       " 'p492',\n",
       " 'p493',\n",
       " 'p494',\n",
       " 'p495',\n",
       " 'p496',\n",
       " 'p497',\n",
       " 'p498',\n",
       " 'p499',\n",
       " 'p500',\n",
       " 'p501',\n",
       " 'p502',\n",
       " 'p503',\n",
       " 'p504',\n",
       " 'p505',\n",
       " 'p506',\n",
       " 'p507',\n",
       " 'p508',\n",
       " 'p509',\n",
       " 'p510',\n",
       " 'p511',\n",
       " 'p512',\n",
       " 'p513',\n",
       " 'p514',\n",
       " 'p515',\n",
       " 'p516',\n",
       " 'p517',\n",
       " 'p518',\n",
       " 'p519',\n",
       " 'p520',\n",
       " 'p521',\n",
       " 'p522',\n",
       " 'p523',\n",
       " 'p524',\n",
       " 'p525',\n",
       " 'p526',\n",
       " 'p527',\n",
       " 'p528',\n",
       " 'p529',\n",
       " 'p530',\n",
       " 'p531',\n",
       " 'p532',\n",
       " 'p533',\n",
       " 'p534',\n",
       " 'p535',\n",
       " 'p536',\n",
       " 'p537',\n",
       " 'p538',\n",
       " 'p539',\n",
       " 'p540',\n",
       " 'p541',\n",
       " 'p542',\n",
       " 'p543',\n",
       " 'p544',\n",
       " 'p545',\n",
       " 'p546',\n",
       " 'p547',\n",
       " 'p548',\n",
       " 'p549',\n",
       " 'p550',\n",
       " 'p551',\n",
       " 'p552',\n",
       " 'p553',\n",
       " 'p554',\n",
       " 'p555',\n",
       " 'p556',\n",
       " 'p557']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df['point_idx'].unique(), key= lambda x: int(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>...</th>\n",
       "      <th>NBRI_diff</th>\n",
       "      <th>BSI_diff</th>\n",
       "      <th>NDWI_diff</th>\n",
       "      <th>NDSI_diff</th>\n",
       "      <th>NDVI_diff</th>\n",
       "      <th>sample_idx</th>\n",
       "      <th>sample_idx_1hot_0</th>\n",
       "      <th>sample_idx_1hot_1</th>\n",
       "      <th>sample_idx_1hot_2</th>\n",
       "      <th>har_evnt_1hot_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>-0.870086</td>\n",
       "      <td>-0.894463</td>\n",
       "      <td>-0.362898</td>\n",
       "      <td>-0.562899</td>\n",
       "      <td>-0.712486</td>\n",
       "      <td>-0.803480</td>\n",
       "      <td>-0.029281</td>\n",
       "      <td>0.751010</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.729749</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.968218</td>\n",
       "      <td>-1.523347</td>\n",
       "      <td>0.521851</td>\n",
       "      <td>1.005915</td>\n",
       "      <td>1.573112</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>-0.312600</td>\n",
       "      <td>-0.369832</td>\n",
       "      <td>-0.086144</td>\n",
       "      <td>0.280409</td>\n",
       "      <td>-0.099855</td>\n",
       "      <td>0.208097</td>\n",
       "      <td>0.629703</td>\n",
       "      <td>0.362093</td>\n",
       "      <td>0.061773</td>\n",
       "      <td>0.152029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906889</td>\n",
       "      <td>-0.333211</td>\n",
       "      <td>1.112289</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>-0.610115</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>0.709577</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-0.152589</td>\n",
       "      <td>-0.109988</td>\n",
       "      <td>-0.493141</td>\n",
       "      <td>-0.196534</td>\n",
       "      <td>1.214141</td>\n",
       "      <td>1.359916</td>\n",
       "      <td>1.234850</td>\n",
       "      <td>1.216424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.719594</td>\n",
       "      <td>-0.579386</td>\n",
       "      <td>-0.135283</td>\n",
       "      <td>0.200484</td>\n",
       "      <td>1.265219</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>-0.561720</td>\n",
       "      <td>-0.722576</td>\n",
       "      <td>-0.312302</td>\n",
       "      <td>-0.473593</td>\n",
       "      <td>-0.729585</td>\n",
       "      <td>-0.671262</td>\n",
       "      <td>0.668467</td>\n",
       "      <td>1.463120</td>\n",
       "      <td>2.055841</td>\n",
       "      <td>1.519354</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.512676</td>\n",
       "      <td>-1.735957</td>\n",
       "      <td>0.806501</td>\n",
       "      <td>1.165113</td>\n",
       "      <td>1.692973</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>-0.299037</td>\n",
       "      <td>-0.194955</td>\n",
       "      <td>-0.324494</td>\n",
       "      <td>-0.356218</td>\n",
       "      <td>-0.293845</td>\n",
       "      <td>-0.373203</td>\n",
       "      <td>-0.754462</td>\n",
       "      <td>-0.892654</td>\n",
       "      <td>-0.904964</td>\n",
       "      <td>-0.916228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231992</td>\n",
       "      <td>0.551830</td>\n",
       "      <td>-0.896480</td>\n",
       "      <td>-0.580907</td>\n",
       "      <td>0.203664</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>-1.223664</td>\n",
       "      <td>-1.653289</td>\n",
       "      <td>-0.874174</td>\n",
       "      <td>-0.848826</td>\n",
       "      <td>-1.376933</td>\n",
       "      <td>-0.955374</td>\n",
       "      <td>1.352084</td>\n",
       "      <td>1.767256</td>\n",
       "      <td>1.749997</td>\n",
       "      <td>1.694362</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.306606</td>\n",
       "      <td>-2.374764</td>\n",
       "      <td>0.411842</td>\n",
       "      <td>2.136983</td>\n",
       "      <td>2.313166</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>0.585052</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>-0.201453</td>\n",
       "      <td>-0.167603</td>\n",
       "      <td>0.086507</td>\n",
       "      <td>-0.184328</td>\n",
       "      <td>-0.739681</td>\n",
       "      <td>-0.749316</td>\n",
       "      <td>-0.777399</td>\n",
       "      <td>-0.679339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>1.137787</td>\n",
       "      <td>-0.727334</td>\n",
       "      <td>-1.122512</td>\n",
       "      <td>-0.399597</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135</th>\n",
       "      <td>-0.186510</td>\n",
       "      <td>0.046045</td>\n",
       "      <td>-0.259642</td>\n",
       "      <td>-0.345125</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>-0.173940</td>\n",
       "      <td>-0.687242</td>\n",
       "      <td>-0.689586</td>\n",
       "      <td>-0.657617</td>\n",
       "      <td>-0.606040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256747</td>\n",
       "      <td>0.560347</td>\n",
       "      <td>-0.615972</td>\n",
       "      <td>-0.461928</td>\n",
       "      <td>-0.459774</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6136</th>\n",
       "      <td>0.421999</td>\n",
       "      <td>1.102268</td>\n",
       "      <td>-0.817404</td>\n",
       "      <td>-1.045111</td>\n",
       "      <td>-0.845186</td>\n",
       "      <td>-1.091576</td>\n",
       "      <td>-1.444705</td>\n",
       "      <td>-1.206955</td>\n",
       "      <td>-1.128188</td>\n",
       "      <td>-1.259767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.749974</td>\n",
       "      <td>1.448490</td>\n",
       "      <td>-2.587140</td>\n",
       "      <td>-1.076003</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>-0.608432</td>\n",
       "      <td>-0.292357</td>\n",
       "      <td>-0.669803</td>\n",
       "      <td>-0.638109</td>\n",
       "      <td>-0.802010</td>\n",
       "      <td>-0.760304</td>\n",
       "      <td>-0.322105</td>\n",
       "      <td>-0.291678</td>\n",
       "      <td>-0.341051</td>\n",
       "      <td>-0.349493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491515</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.655980</td>\n",
       "      <td>0.251211</td>\n",
       "      <td>0.849334</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2083 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B11       B12        B2        B3        B4        B5        B6  \\\n",
       "3906 -0.870086 -0.894463 -0.362898 -0.562899 -0.712486 -0.803480 -0.029281   \n",
       "3907 -0.312600 -0.369832 -0.086144  0.280409 -0.099855  0.208097  0.629703   \n",
       "3908  0.709577  0.053909 -0.152589 -0.109988 -0.493141 -0.196534  1.214141   \n",
       "3909 -0.561720 -0.722576 -0.312302 -0.473593 -0.729585 -0.671262  0.668467   \n",
       "3910 -0.299037 -0.194955 -0.324494 -0.356218 -0.293845 -0.373203 -0.754462   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6133 -1.223664 -1.653289 -0.874174 -0.848826 -1.376933 -0.955374  1.352084   \n",
       "6134  0.585052  0.995327 -0.201453 -0.167603  0.086507 -0.184328 -0.739681   \n",
       "6135 -0.186510  0.046045 -0.259642 -0.345125  0.100141 -0.173940 -0.687242   \n",
       "6136  0.421999  1.102268 -0.817404 -1.045111 -0.845186 -1.091576 -1.444705   \n",
       "6137 -0.608432 -0.292357 -0.669803 -0.638109 -0.802010 -0.760304 -0.322105   \n",
       "\n",
       "            B7        B8       B8A  ...  NBRI_diff  BSI_diff  NDWI_diff  \\\n",
       "3906  0.751010  0.946631  0.729749  ...  -2.968218 -1.523347   0.521851   \n",
       "3907  0.362093  0.061773  0.152029  ...   0.906889 -0.333211   1.112289   \n",
       "3908  1.359916  1.234850  1.216424  ...  -0.719594 -0.579386  -0.135283   \n",
       "3909  1.463120  2.055841  1.519354  ...  -3.512676 -1.735957   0.806501   \n",
       "3910 -0.892654 -0.904964 -0.916228  ...   0.231992  0.551830  -0.896480   \n",
       "...        ...       ...       ...  ...        ...       ...        ...   \n",
       "6133  1.767256  1.749997  1.694362  ...  -3.306606 -2.374764   0.411842   \n",
       "6134 -0.749316 -0.777399 -0.679339  ...   0.397156  1.137787  -0.727334   \n",
       "6135 -0.689586 -0.657617 -0.606040  ...   0.256747  0.560347  -0.615972   \n",
       "6136 -1.206955 -1.128188 -1.259767  ...  -0.749974  1.448490  -2.587140   \n",
       "6137 -0.291678 -0.341051 -0.349493  ...  -0.491515 -0.017271  -0.655980   \n",
       "\n",
       "      NDSI_diff NDVI_diff sample_idx sample_idx_1hot_0  sample_idx_1hot_1  \\\n",
       "3906   1.005915  1.573112         s6               0.0                0.0   \n",
       "3907   0.629439 -0.610115         s6               0.0                0.0   \n",
       "3908   0.200484  1.265219         s6               0.0                0.0   \n",
       "3909   1.165113  1.692973         s6               0.0                0.0   \n",
       "3910  -0.580907  0.203664         s6               0.0                0.0   \n",
       "...         ...       ...        ...               ...                ...   \n",
       "6133   2.136983  2.313166         s9               0.0                0.0   \n",
       "6134  -1.122512 -0.399597         s9               0.0                0.0   \n",
       "6135  -0.461928 -0.459774         s9               0.0                0.0   \n",
       "6136  -1.076003  0.980113         s9               0.0                0.0   \n",
       "6137   0.251211  0.849334         s9               0.0                0.0   \n",
       "\n",
       "     sample_idx_1hot_2 har_evnt_1hot_0  \n",
       "3906               0.0             0.0  \n",
       "3907               0.0             0.0  \n",
       "3908               0.0             0.0  \n",
       "3909               0.0             0.0  \n",
       "3910               0.0             0.0  \n",
       "...                ...             ...  \n",
       "6133               1.0             0.0  \n",
       "6134               1.0             0.0  \n",
       "6135               1.0             0.0  \n",
       "6136               1.0             0.0  \n",
       "6137               1.0             0.0  \n",
       "\n",
       "[2083 rows x 70 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(2, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(3, 70)\n",
      "(1, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(1, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(2, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(3, 70)\n",
      "(4, 70)\n",
      "(4, 70)\n"
     ]
    }
   ],
   "source": [
    "#  reshape the data into a 3D tensor with shape X (num_samples, sequence_length, num_features) to feed it into a 1D CNN in TensorFlow for training.\n",
    "# sequence_length: each 3-week sample\n",
    "# num_samples: num of points, aka farms\n",
    "\n",
    "points_dfs_X = []\n",
    "points_dfs_Y = []\n",
    "\n",
    "for p in sorted(df['point_idx'].unique(), key= lambda x: int(x[1:])):\n",
    "    curr_df = df[df[\"point_idx\"] == p]\n",
    "    print(curr_df.shape)\n",
    "    if(curr_df.shape[0] == 4): #  TODO fix this, even method1 is not done correctly :/ \n",
    "        \"\"\"\n",
    "        TODO\n",
    "        use one of the follwoing(currently using method 1):\n",
    "            1-Remove the rows with missing values.\n",
    "            2-Fill in missing values with a constant\n",
    "            3-Interpolate missing values\n",
    "            4-Use imputation methods\n",
    "        \"\"\"\n",
    "        curr_df_X = (curr_df[one_hot_names_X + BANDS]).to_numpy()\n",
    "        points_dfs_X.append(curr_df_X) \n",
    "\n",
    "        curr_df_Y = (curr_df[one_hot_names_Y]).to_numpy()\n",
    "        points_dfs_Y.append(curr_df_Y) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>...</th>\n",
       "      <th>NBRI_diff</th>\n",
       "      <th>BSI_diff</th>\n",
       "      <th>NDWI_diff</th>\n",
       "      <th>NDSI_diff</th>\n",
       "      <th>NDVI_diff</th>\n",
       "      <th>sample_idx</th>\n",
       "      <th>sample_idx_1hot_0</th>\n",
       "      <th>sample_idx_1hot_1</th>\n",
       "      <th>sample_idx_1hot_2</th>\n",
       "      <th>har_evnt_1hot_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>0.753119</td>\n",
       "      <td>0.641316</td>\n",
       "      <td>-0.212329</td>\n",
       "      <td>-0.171226</td>\n",
       "      <td>-0.158228</td>\n",
       "      <td>-0.093951</td>\n",
       "      <td>0.024988</td>\n",
       "      <td>-0.081143</td>\n",
       "      <td>-0.080699</td>\n",
       "      <td>-0.206632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013575</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>-1.117648</td>\n",
       "      <td>-0.653725</td>\n",
       "      <td>0.550355</td>\n",
       "      <td>s6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>0.080242</td>\n",
       "      <td>-0.205906</td>\n",
       "      <td>-0.272849</td>\n",
       "      <td>-0.285226</td>\n",
       "      <td>-0.706196</td>\n",
       "      <td>-0.449542</td>\n",
       "      <td>0.527871</td>\n",
       "      <td>0.564223</td>\n",
       "      <td>0.577029</td>\n",
       "      <td>0.496656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.749229</td>\n",
       "      <td>-0.645731</td>\n",
       "      <td>-0.328488</td>\n",
       "      <td>0.509959</td>\n",
       "      <td>1.463451</td>\n",
       "      <td>s7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>-0.417185</td>\n",
       "      <td>-0.272877</td>\n",
       "      <td>-0.455992</td>\n",
       "      <td>-0.445247</td>\n",
       "      <td>-0.745832</td>\n",
       "      <td>-0.453823</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.245941</td>\n",
       "      <td>0.352068</td>\n",
       "      <td>0.204620</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.745090</td>\n",
       "      <td>-0.895212</td>\n",
       "      <td>-0.536235</td>\n",
       "      <td>1.144411</td>\n",
       "      <td>1.712407</td>\n",
       "      <td>s8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>-0.608432</td>\n",
       "      <td>-0.292357</td>\n",
       "      <td>-0.669803</td>\n",
       "      <td>-0.638109</td>\n",
       "      <td>-0.802010</td>\n",
       "      <td>-0.760304</td>\n",
       "      <td>-0.322105</td>\n",
       "      <td>-0.291678</td>\n",
       "      <td>-0.341051</td>\n",
       "      <td>-0.349493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491515</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.655980</td>\n",
       "      <td>0.251211</td>\n",
       "      <td>0.849334</td>\n",
       "      <td>s9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B11       B12        B2        B3        B4        B5        B6  \\\n",
       "4463  0.753119  0.641316 -0.212329 -0.171226 -0.158228 -0.093951  0.024988   \n",
       "5021  0.080242 -0.205906 -0.272849 -0.285226 -0.706196 -0.449542  0.527871   \n",
       "5579 -0.417185 -0.272877 -0.455992 -0.445247 -0.745832 -0.453823  0.228387   \n",
       "6137 -0.608432 -0.292357 -0.669803 -0.638109 -0.802010 -0.760304 -0.322105   \n",
       "\n",
       "            B7        B8       B8A  ...  NBRI_diff  BSI_diff  NDWI_diff  \\\n",
       "4463 -0.081143 -0.080699 -0.206632  ...  -0.013575  0.607005  -1.117648   \n",
       "5021  0.564223  0.577029  0.496656  ...  -0.749229 -0.645731  -0.328488   \n",
       "5579  0.245941  0.352068  0.204620  ...  -1.745090 -0.895212  -0.536235   \n",
       "6137 -0.291678 -0.341051 -0.349493  ...  -0.491515 -0.017271  -0.655980   \n",
       "\n",
       "      NDSI_diff NDVI_diff sample_idx sample_idx_1hot_0  sample_idx_1hot_1  \\\n",
       "4463  -0.653725  0.550355         s6               0.0                0.0   \n",
       "5021   0.509959  1.463451         s7               1.0                0.0   \n",
       "5579   1.144411  1.712407         s8               0.0                1.0   \n",
       "6137   0.251211  0.849334         s9               0.0                0.0   \n",
       "\n",
       "     sample_idx_1hot_2 har_evnt_1hot_0  \n",
       "4463               0.0             0.0  \n",
       "5021               0.0             0.0  \n",
       "5579               0.0             0.0  \n",
       "6137               1.0             0.0  \n",
       "\n",
       "[4 rows x 70 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(points_dfs_X)\n",
    "Y = np.array(points_dfs_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430, 4, 13)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430, 4, 1)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.reshape((430, 4))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(4, 13)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=4)\n",
    "])\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 2s 28ms/step - loss: 0.1411 - accuracy: 0.3566 - val_loss: 0.1068 - val_accuracy: 0.3023\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1111 - accuracy: 0.4289 - val_loss: 0.1056 - val_accuracy: 0.3488\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.1055 - accuracy: 0.4315 - val_loss: 0.1053 - val_accuracy: 0.3256\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0978 - accuracy: 0.4496 - val_loss: 0.1055 - val_accuracy: 0.3256\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0928 - accuracy: 0.4574 - val_loss: 0.0984 - val_accuracy: 0.3256\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0890 - accuracy: 0.4987 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.4935 - val_loss: 0.0943 - val_accuracy: 0.3256\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0861 - accuracy: 0.4935 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.0795 - accuracy: 0.5013 - val_loss: 0.0878 - val_accuracy: 0.4186\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0768 - accuracy: 0.5349 - val_loss: 0.0811 - val_accuracy: 0.3488\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.5297 - val_loss: 0.0900 - val_accuracy: 0.3721\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0752 - accuracy: 0.5711 - val_loss: 0.0843 - val_accuracy: 0.3488\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0701 - accuracy: 0.5426 - val_loss: 0.0838 - val_accuracy: 0.4186\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.5633 - val_loss: 0.0831 - val_accuracy: 0.3488\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0640 - accuracy: 0.5711 - val_loss: 0.0855 - val_accuracy: 0.3721\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0633 - accuracy: 0.5995 - val_loss: 0.0821 - val_accuracy: 0.3488\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0631 - accuracy: 0.5762 - val_loss: 0.0814 - val_accuracy: 0.3721\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.5659 - val_loss: 0.0882 - val_accuracy: 0.3721\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0617 - accuracy: 0.6021 - val_loss: 0.0839 - val_accuracy: 0.4186\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.5995 - val_loss: 0.0924 - val_accuracy: 0.3953\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0513 - accuracy: 0.6176 - val_loss: 0.0950 - val_accuracy: 0.3721\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0493 - accuracy: 0.6253 - val_loss: 0.0971 - val_accuracy: 0.3721\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.5814 - val_loss: 0.0917 - val_accuracy: 0.3953\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.0579 - accuracy: 0.5891 - val_loss: 0.0923 - val_accuracy: 0.3953\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0507 - accuracy: 0.6021 - val_loss: 0.0925 - val_accuracy: 0.3953\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0522 - accuracy: 0.5866 - val_loss: 0.0974 - val_accuracy: 0.3488\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0495 - accuracy: 0.6150 - val_loss: 0.1028 - val_accuracy: 0.3721\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.6150 - val_loss: 0.0880 - val_accuracy: 0.3721\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0459 - accuracy: 0.6176 - val_loss: 0.0832 - val_accuracy: 0.4186\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0521 - accuracy: 0.6072 - val_loss: 0.1020 - val_accuracy: 0.3256\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.6305 - val_loss: 0.0924 - val_accuracy: 0.3953\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.6253 - val_loss: 0.0989 - val_accuracy: 0.3953\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0394 - accuracy: 0.6718 - val_loss: 0.0969 - val_accuracy: 0.3721\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.6279 - val_loss: 0.1047 - val_accuracy: 0.3953\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0480 - accuracy: 0.6382 - val_loss: 0.0979 - val_accuracy: 0.4651\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.6512 - val_loss: 0.0985 - val_accuracy: 0.4186\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0405 - accuracy: 0.6563 - val_loss: 0.0998 - val_accuracy: 0.4186\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0396 - accuracy: 0.6279 - val_loss: 0.0983 - val_accuracy: 0.4186\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0389 - accuracy: 0.6718 - val_loss: 0.0954 - val_accuracy: 0.4186\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0347 - accuracy: 0.6848 - val_loss: 0.1008 - val_accuracy: 0.3953\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.0348 - accuracy: 0.6589 - val_loss: 0.0967 - val_accuracy: 0.4186\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0339 - accuracy: 0.6822 - val_loss: 0.0992 - val_accuracy: 0.4186\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0301 - accuracy: 0.6641 - val_loss: 0.0933 - val_accuracy: 0.4186\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0290 - accuracy: 0.6796 - val_loss: 0.1072 - val_accuracy: 0.3953\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.6848 - val_loss: 0.0953 - val_accuracy: 0.4186\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0388 - accuracy: 0.6563 - val_loss: 0.1032 - val_accuracy: 0.3488\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0342 - accuracy: 0.6589 - val_loss: 0.1107 - val_accuracy: 0.3953\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.6486 - val_loss: 0.1053 - val_accuracy: 0.3721\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0383 - accuracy: 0.6357 - val_loss: 0.0917 - val_accuracy: 0.3721\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0329 - accuracy: 0.6822 - val_loss: 0.1087 - val_accuracy: 0.3721\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.6382 - val_loss: 0.1183 - val_accuracy: 0.3721\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0468 - accuracy: 0.6176 - val_loss: 0.0917 - val_accuracy: 0.3953\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0413 - accuracy: 0.6537 - val_loss: 0.1181 - val_accuracy: 0.3953\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0354 - accuracy: 0.6537 - val_loss: 0.1002 - val_accuracy: 0.3953\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.6537 - val_loss: 0.1245 - val_accuracy: 0.3721\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0298 - accuracy: 0.6718 - val_loss: 0.1007 - val_accuracy: 0.3953\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0306 - accuracy: 0.6615 - val_loss: 0.1095 - val_accuracy: 0.3721\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0312 - accuracy: 0.6641 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0282 - accuracy: 0.6563 - val_loss: 0.1224 - val_accuracy: 0.3953\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0286 - accuracy: 0.6589 - val_loss: 0.0969 - val_accuracy: 0.3721\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 0.6667 - val_loss: 0.1039 - val_accuracy: 0.4186\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 0.6615 - val_loss: 0.1144 - val_accuracy: 0.3721\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.6796 - val_loss: 0.1074 - val_accuracy: 0.3953\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0220 - accuracy: 0.6770 - val_loss: 0.1211 - val_accuracy: 0.3256\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0314 - accuracy: 0.6357 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0312 - accuracy: 0.6512 - val_loss: 0.1203 - val_accuracy: 0.3953\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0246 - accuracy: 0.6641 - val_loss: 0.1104 - val_accuracy: 0.3488\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0263 - accuracy: 0.6589 - val_loss: 0.1162 - val_accuracy: 0.3488\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0290 - accuracy: 0.6434 - val_loss: 0.1241 - val_accuracy: 0.3721\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0318 - accuracy: 0.6460 - val_loss: 0.1101 - val_accuracy: 0.3953\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0313 - accuracy: 0.6667 - val_loss: 0.1081 - val_accuracy: 0.3488\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0253 - accuracy: 0.6693 - val_loss: 0.1128 - val_accuracy: 0.3953\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0278 - accuracy: 0.6693 - val_loss: 0.1105 - val_accuracy: 0.3488\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0227 - accuracy: 0.6693 - val_loss: 0.0970 - val_accuracy: 0.3488\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0248 - accuracy: 0.6667 - val_loss: 0.1065 - val_accuracy: 0.3488\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0242 - accuracy: 0.6822 - val_loss: 0.1056 - val_accuracy: 0.3721\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0215 - accuracy: 0.6744 - val_loss: 0.0998 - val_accuracy: 0.3953\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 0.6434 - val_loss: 0.0929 - val_accuracy: 0.3953\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 0.6615 - val_loss: 0.0964 - val_accuracy: 0.4186\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.6796 - val_loss: 0.0934 - val_accuracy: 0.3721\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0286 - accuracy: 0.6667 - val_loss: 0.0968 - val_accuracy: 0.4186\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0253 - accuracy: 0.6641 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0264 - accuracy: 0.6718 - val_loss: 0.0979 - val_accuracy: 0.4186\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.6331 - val_loss: 0.1096 - val_accuracy: 0.4186\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0341 - accuracy: 0.6434 - val_loss: 0.1029 - val_accuracy: 0.4419\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.6434 - val_loss: 0.0921 - val_accuracy: 0.4186\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0257 - accuracy: 0.6537 - val_loss: 0.0961 - val_accuracy: 0.4186\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0226 - accuracy: 0.6615 - val_loss: 0.1049 - val_accuracy: 0.3953\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0210 - accuracy: 0.6693 - val_loss: 0.1056 - val_accuracy: 0.3953\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0229 - accuracy: 0.6615 - val_loss: 0.1156 - val_accuracy: 0.3953\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.0265 - accuracy: 0.6641 - val_loss: 0.0991 - val_accuracy: 0.3953\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0220 - accuracy: 0.6718 - val_loss: 0.1106 - val_accuracy: 0.3953\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0190 - accuracy: 0.6822 - val_loss: 0.0956 - val_accuracy: 0.4186\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0205 - accuracy: 0.6873 - val_loss: 0.1072 - val_accuracy: 0.3721\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.6770 - val_loss: 0.1021 - val_accuracy: 0.3953\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0238 - accuracy: 0.6718 - val_loss: 0.1040 - val_accuracy: 0.3488\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0252 - accuracy: 0.6744 - val_loss: 0.1098 - val_accuracy: 0.3721\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.6822 - val_loss: 0.1128 - val_accuracy: 0.3721\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0193 - accuracy: 0.6977 - val_loss: 0.1139 - val_accuracy: 0.3953\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0213 - accuracy: 0.6770 - val_loss: 0.1084 - val_accuracy: 0.3953\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0183 - accuracy: 0.6822 - val_loss: 0.1085 - val_accuracy: 0.3721\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0152 - accuracy: 0.6951 - val_loss: 0.0906 - val_accuracy: 0.3721\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 0.6899 - val_loss: 0.1044 - val_accuracy: 0.3721\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0168 - accuracy: 0.6693 - val_loss: 0.1056 - val_accuracy: 0.3721\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.6589 - val_loss: 0.1068 - val_accuracy: 0.4186\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1115 - val_accuracy: 0.3721\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0270 - accuracy: 0.6563 - val_loss: 0.0903 - val_accuracy: 0.4186\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0224 - accuracy: 0.6615 - val_loss: 0.0975 - val_accuracy: 0.3953\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1005 - val_accuracy: 0.3953\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0210 - accuracy: 0.6667 - val_loss: 0.1023 - val_accuracy: 0.3721\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0221 - accuracy: 0.6641 - val_loss: 0.1012 - val_accuracy: 0.3721\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0183 - accuracy: 0.6744 - val_loss: 0.1073 - val_accuracy: 0.3953\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0202 - accuracy: 0.6822 - val_loss: 0.1000 - val_accuracy: 0.3953\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0171 - accuracy: 0.6873 - val_loss: 0.0926 - val_accuracy: 0.3721\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0192 - accuracy: 0.6925 - val_loss: 0.0979 - val_accuracy: 0.3721\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0217 - accuracy: 0.6615 - val_loss: 0.0911 - val_accuracy: 0.3953\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0168 - accuracy: 0.7158 - val_loss: 0.0870 - val_accuracy: 0.3953\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0214 - accuracy: 0.6977 - val_loss: 0.0858 - val_accuracy: 0.3721\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 0.6770 - val_loss: 0.1159 - val_accuracy: 0.3256\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.6641 - val_loss: 0.0966 - val_accuracy: 0.4186\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0151 - accuracy: 0.6796 - val_loss: 0.0949 - val_accuracy: 0.3953\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0166 - accuracy: 0.6744 - val_loss: 0.1064 - val_accuracy: 0.3953\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 0.6641 - val_loss: 0.1025 - val_accuracy: 0.3953\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0136 - accuracy: 0.6925 - val_loss: 0.1081 - val_accuracy: 0.3721\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0135 - accuracy: 0.6770 - val_loss: 0.1109 - val_accuracy: 0.3721\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0131 - accuracy: 0.6873 - val_loss: 0.1076 - val_accuracy: 0.3721\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 0.6744 - val_loss: 0.1080 - val_accuracy: 0.3488\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.6873 - val_loss: 0.1058 - val_accuracy: 0.3488\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0130 - accuracy: 0.6873 - val_loss: 0.0960 - val_accuracy: 0.3953\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.7106 - val_loss: 0.0902 - val_accuracy: 0.4186\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0128 - accuracy: 0.6951 - val_loss: 0.0959 - val_accuracy: 0.4186\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0114 - accuracy: 0.7003 - val_loss: 0.0991 - val_accuracy: 0.4186\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0130 - accuracy: 0.7442 - val_loss: 0.0982 - val_accuracy: 0.3721\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0168 - accuracy: 0.7158 - val_loss: 0.0983 - val_accuracy: 0.4186\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0194 - accuracy: 0.7287 - val_loss: 0.1103 - val_accuracy: 0.4186\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.0186 - accuracy: 0.6744 - val_loss: 0.1014 - val_accuracy: 0.3953\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 0.6925 - val_loss: 0.1151 - val_accuracy: 0.3488\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0244 - accuracy: 0.6537 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0231 - accuracy: 0.6848 - val_loss: 0.1113 - val_accuracy: 0.3488\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.6796 - val_loss: 0.1017 - val_accuracy: 0.3953\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 0.6925 - val_loss: 0.1142 - val_accuracy: 0.3488\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.7132 - val_loss: 0.1087 - val_accuracy: 0.3721\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0143 - accuracy: 0.6848 - val_loss: 0.1020 - val_accuracy: 0.4186\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0192 - accuracy: 0.6899 - val_loss: 0.1054 - val_accuracy: 0.3721\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0176 - accuracy: 0.6744 - val_loss: 0.1077 - val_accuracy: 0.3488\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 0.6615 - val_loss: 0.0982 - val_accuracy: 0.3721\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1045 - val_accuracy: 0.3488\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0160 - accuracy: 0.6693 - val_loss: 0.0901 - val_accuracy: 0.3953\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0151 - accuracy: 0.6899 - val_loss: 0.0989 - val_accuracy: 0.3953\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.6977 - val_loss: 0.1064 - val_accuracy: 0.3721\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.6770 - val_loss: 0.0989 - val_accuracy: 0.3721\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0142 - accuracy: 0.6770 - val_loss: 0.0984 - val_accuracy: 0.3488\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0192 - accuracy: 0.6822 - val_loss: 0.1019 - val_accuracy: 0.3721\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0203 - accuracy: 0.6718 - val_loss: 0.0936 - val_accuracy: 0.3953\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0177 - accuracy: 0.6770 - val_loss: 0.1021 - val_accuracy: 0.3953\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 0.6718 - val_loss: 0.1054 - val_accuracy: 0.3721\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0223 - accuracy: 0.6641 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0201 - accuracy: 0.6770 - val_loss: 0.1009 - val_accuracy: 0.3721\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0244 - accuracy: 0.6537 - val_loss: 0.1031 - val_accuracy: 0.3721\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.6693 - val_loss: 0.1004 - val_accuracy: 0.3721\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.0203 - accuracy: 0.6693 - val_loss: 0.1058 - val_accuracy: 0.3721\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.6641 - val_loss: 0.1058 - val_accuracy: 0.3488\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0150 - accuracy: 0.6848 - val_loss: 0.1064 - val_accuracy: 0.3953\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0114 - accuracy: 0.6951 - val_loss: 0.1135 - val_accuracy: 0.3488\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6977 - val_loss: 0.1092 - val_accuracy: 0.3488\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0119 - accuracy: 0.6899 - val_loss: 0.1078 - val_accuracy: 0.3721\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0120 - accuracy: 0.7028 - val_loss: 0.0966 - val_accuracy: 0.3488\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.6770 - val_loss: 0.1033 - val_accuracy: 0.3488\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 0.6770 - val_loss: 0.1150 - val_accuracy: 0.3488\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.6899 - val_loss: 0.1035 - val_accuracy: 0.3721\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0122 - accuracy: 0.6925 - val_loss: 0.1054 - val_accuracy: 0.3488\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 0.6873 - val_loss: 0.1127 - val_accuracy: 0.3488\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0138 - accuracy: 0.6770 - val_loss: 0.1108 - val_accuracy: 0.3721\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0116 - accuracy: 0.6822 - val_loss: 0.0975 - val_accuracy: 0.3721\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.6796 - val_loss: 0.0930 - val_accuracy: 0.4186\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6770 - val_loss: 0.0962 - val_accuracy: 0.3953\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 0.6977 - val_loss: 0.1032 - val_accuracy: 0.3953\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.6796 - val_loss: 0.0945 - val_accuracy: 0.3953\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.6848 - val_loss: 0.0936 - val_accuracy: 0.3953\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0129 - accuracy: 0.6873 - val_loss: 0.0956 - val_accuracy: 0.3488\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 0.6925 - val_loss: 0.0964 - val_accuracy: 0.3721\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0108 - accuracy: 0.6796 - val_loss: 0.1047 - val_accuracy: 0.3488\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0131 - accuracy: 0.6873 - val_loss: 0.0979 - val_accuracy: 0.3721\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0116 - accuracy: 0.6744 - val_loss: 0.1026 - val_accuracy: 0.3721\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.6693 - val_loss: 0.0938 - val_accuracy: 0.3721\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0092 - accuracy: 0.6977 - val_loss: 0.0986 - val_accuracy: 0.3488\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0155 - accuracy: 0.6770 - val_loss: 0.0967 - val_accuracy: 0.3488\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 0.6822 - val_loss: 0.0970 - val_accuracy: 0.3488\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 0.6822 - val_loss: 0.0941 - val_accuracy: 0.3721\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0113 - accuracy: 0.6848 - val_loss: 0.1010 - val_accuracy: 0.3488\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0101 - accuracy: 0.6951 - val_loss: 0.0916 - val_accuracy: 0.3721\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0102 - accuracy: 0.6873 - val_loss: 0.0897 - val_accuracy: 0.3488\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 0.6718 - val_loss: 0.0957 - val_accuracy: 0.3488\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0122 - accuracy: 0.6848 - val_loss: 0.0982 - val_accuracy: 0.3488\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6899 - val_loss: 0.1066 - val_accuracy: 0.3488\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.6873 - val_loss: 0.0988 - val_accuracy: 0.3721\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0119 - accuracy: 0.6848 - val_loss: 0.1081 - val_accuracy: 0.3721\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.6848 - val_loss: 0.1109 - val_accuracy: 0.3721\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.0119 - accuracy: 0.6848 - val_loss: 0.1120 - val_accuracy: 0.3488\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0075 - accuracy: 0.6848 - val_loss: 0.1104 - val_accuracy: 0.3488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbff228d760>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=200, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , ...,  0.00552828,\n",
       "         -0.00659141, -0.00677609],\n",
       "        [ 0.00757559,  0.        ,  0.        , ..., -0.00238965,\n",
       "         -0.01646883, -0.01451381],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00472415,\n",
       "         -0.00212759, -0.00230309],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ...,  0.00232412,\n",
       "          0.00348978,  0.00299009]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.00921513,\n",
       "          0.00537546,  0.00040839],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.0002729 ,\n",
       "          0.00560671,  0.00508392],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00203002,\n",
       "          0.00129858,  0.00159386],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ...,  0.01397219,\n",
       "         -0.00468563, -0.00910651]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.01151   ,\n",
       "         -0.00425536, -0.00547394],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00533579,\n",
       "         -0.00712918, -0.00816971],\n",
       "        [ 0.        ,  0.00757559,  0.        , ...,  0.00495386,\n",
       "          0.00266738,  0.00054649],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00891716,\n",
       "          0.00700444,  0.01336099]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.00381007,\n",
       "          0.00246621,  0.00270697],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00583502,\n",
       "          0.01261146,  0.01024062],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00509299,\n",
       "          0.00385369,  0.00856684],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00514639,\n",
       "          0.00443211,  0.00754019]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.04468748,\n",
       "          0.01935934,  0.00836849],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00039393,\n",
       "         -0.00369049, -0.00365194],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00803568,\n",
       "         -0.00708019, -0.00604907],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00954347,\n",
       "          0.00319689,  0.00835033]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.00156536,\n",
       "          0.00570532,  0.00485835],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00376246,\n",
       "          0.00060788, -0.00155986],\n",
       "        [ 0.        ,  0.00757559,  0.        , ...,  0.00155012,\n",
       "         -0.00316042, -0.0020672 ],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00264761,\n",
       "         -0.00460923, -0.00221478]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(X)\n",
    "tf.math.l2_normalize(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1249, 57), found shape=(None, 57)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m trainX, testX, trainy, testy \u001b[39m=\u001b[39m  train_test_split(X, Y, test_size \u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m) \n\u001b[0;32m----> 5\u001b[0m evaluate_model(trainX, trainy, testX, testy)\n",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(trainX, trainy, testX, testy)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[39m# fit network\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainy, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     15\u001b[0m \u001b[39m# evaluate model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m _, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(testX, testy, batch_size\u001b[39m=\u001b[39mbatch_size, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/6q/tw4w22c97c98vxdk2crxzdb00000gn/T/__autograph_generated_fileemzwtj1w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1249, 57), found shape=(None, 57)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy =  train_test_split(X, Y, test_size = 0.4,random_state=42) \n",
    "\n",
    "evaluate_model(trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1249, 1), (834, 57))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape, trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvest-event-detec-Fyp2lGEN-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

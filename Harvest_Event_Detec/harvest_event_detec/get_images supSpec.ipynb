{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import google.auth\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "import time\n",
    "import geopandas\n",
    "import pandas\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed to Successfully save authorization token. from ee.Authenticate()\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=SCF1c1rNCpPcNmuspV7tKarL9y8xbezaTM5ZSR-D69g&tc=Sn-GWA2Kfiq01JYd6gnrY82rGR7L9KnHszY3znnOfHI&cc=BS-tXBW1TJwXseeehsBy1lGvExVIVHxnVdfD106ICg0>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=SCF1c1rNCpPcNmuspV7tKarL9y8xbezaTM5ZSR-D69g&tc=Sn-GWA2Kfiq01JYd6gnrY82rGR7L9KnHszY3znnOfHI&cc=BS-tXBW1TJwXseeehsBy1lGvExVIVHxnVdfD106ICg0</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands used for prediction\n",
    "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'] # bands with <= 30 resolution\n",
    "OVERLAYED_FOLDER_NAMES = [\"overlayed_3week_images_validation/\", \"overlayed_3week_images_train/\"]\n",
    "MERGED_FILE_NAMES = [\"merged_images_validation\", \"merged_images_train\"]\n",
    "\n",
    "\n",
    "COUNTRY_GEOMETRY = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\").filter(ee.Filter.eq('country_na', 'Ukraine'))\n",
    "COUNTRY_LATLON = 50., 31 # coordinate of center of ukraine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Labeled Shape File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_harvested        lon        lat finHarvDat  point_id\n",
      "0         False  33.750930  46.347320   13/06/22         0\n",
      "1         False  28.990823  46.278183   13/06/22         1\n",
      "2         False  33.093223  47.073863   13/06/22         2\n",
      "3         False  38.596773  49.455416   13/06/22         3\n",
      "4         False  33.723420  48.101587   13/06/22         4 (18316, 5)\n"
     ]
    }
   ],
   "source": [
    "#%%script echo skipping\n",
    "OVERLAYED_FOLDER_NAME = OVERLAYED_FOLDER_NAMES[1]\n",
    "MERGED_FILE_NAME = MERGED_FILE_NAMES[1]\n",
    "\n",
    "TIMESTAMP_DIC ={\n",
    "                \"1\" : \"13/06/22\",\n",
    "                \"2\" : \"27/06/22\",\n",
    "                \"3\" : \"11/07/22\",\n",
    "                '4' :  \"25/07/22\",\n",
    "                '5' : \"08/08/22\",\n",
    "                '6' :  \"22/08/22\",\n",
    "                '7' :  \"05/09/22\",\n",
    "                '8' :  \"19/10/22\",\n",
    "                }\n",
    "\n",
    "coor_points_df = None\n",
    "for i in range(1, 8):\n",
    "    curr_df = pandas.read_csv(f\"../data/HarvestSupervised2/supSpectralBands_{i}.csv\")[[\"0_constant\", \".geo\"]]\n",
    "    curr_df[\"0_constant\"] = curr_df[\"0_constant\"].astype(str)\n",
    "    curr_df[\"is_harvested\"] = curr_df[\"0_constant\"].apply(lambda x : str(x) != \"0\")\n",
    "    curr_df[\"0_constant\"].mask(curr_df[\"0_constant\"] == \"0\", str(i), inplace=True)\n",
    "    if(type(coor_points_df) == type(None)):\n",
    "        coor_points_df = curr_df\n",
    "    else:\n",
    "        coor_points_df = pandas.concat([coor_points_df, curr_df])\n",
    "    \n",
    "#coor_points_df[\".geo\"] = coor_points_df[\".geo\"].astype(str)\n",
    "#def get_lat(x):\n",
    "    \n",
    "coor_points_df[\"lon\"] = coor_points_df[\".geo\"].apply(lambda x: ast.literal_eval(x)[\"coordinates\"][0])\n",
    "coor_points_df[\"lat\"] = coor_points_df[\".geo\"].apply(lambda x: ast.literal_eval(x)[\"coordinates\"][1])\n",
    "\n",
    "coor_points_df[\"finHarvDat\"] = coor_points_df[\"0_constant\"].apply(lambda x: TIMESTAMP_DIC[str(x)])\n",
    "coor_points_df.finHarvDat = coor_points_df.finHarvDat.apply(lambda x: str(x))\n",
    "coor_points_df[\"point_id\"] = np.arange(0, coor_points_df.shape[0], 1, dtype=int)\n",
    "coor_points_df = coor_points_df.drop([\".geo\", \"0_constant\"], axis=1)\n",
    "print(coor_points_df.head(), coor_points_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "OVERLAYED_FOLDER_NAME = OVERLAYED_FOLDER_NAMES[0]\n",
    "MERGED_FILE_NAME = MERGED_FILE_NAMES[0]\n",
    "\n",
    "shapeFile = geopandas.read_file(\"../data/validation_data/merged_harvest_validation_20220919.shp\")\n",
    "shapeFile.head()\n",
    "\n",
    "coor_points_df = shapeFile[['lat', 'lon', 'val_set1', 'finHarvDat']].dropna(subset=['lat', 'lon', 'val_set1'])\n",
    "coor_points_df['is_harvested'] = coor_points_df['val_set1'].apply(lambda x: x == 1)\n",
    "coor_points_df = coor_points_df.drop(['val_set1'], axis=1)\n",
    "coor_points_df.finHarvDat = coor_points_df.finHarvDat.apply(lambda x: str(x))\n",
    "coor_points_df['point_id'] = np.arange(0, coor_points_df.shape[0], 1, dtype=int)\n",
    "print(coor_points_df.head(), coor_points_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_points(img: ee.Image, df:pandas.DataFrame, coordinate_col_names:(str, str)=('lon', 'lat')) -> ee.FeatureCollection:\n",
    "  \"\"\"\n",
    "  Overlays the points at df onto img. then, creates a table with the overlayed points(saved as ee.FeatureCollection).\n",
    "  The ee.FeatureCollection is exported into Drive in a GeoJson format.\n",
    "  So, the bands reflectances from img are put into the dataset described as the dataframe(df), then exported as .GeoJson.\n",
    "  \n",
    "  Args:\n",
    "      img (ee.Image): _description_\n",
    "      df (pandas.DataFrame): cointains longitude and latitude cols, describing the points to be overlayed.\n",
    "      coordinate_col_names (str, str, optional): the name of columns (in the dataframe df) that have the coordinates. Defaults to ('lon', 'lat').\n",
    "\n",
    "  Returns:\n",
    "      ee.FeatureCollection: _description_\n",
    "      \n",
    "  Usage:\n",
    "      export_to_drive(overlay_points(image, coor_points_df), 'points_from_sph')\n",
    "      # will save points_from_sph.geojson into drive.\n",
    "  \"\"\"\n",
    "  # Convert pandas dataframe to an ee.FeatureCollection\n",
    "  def createFeature(row):\n",
    "      lon = coordinate_col_names[0]\n",
    "      lat = coordinate_col_names[1]\n",
    "      geometry = ee.Geometry.Point([row[lon], row[lat]])\n",
    "      #print(df.columns.values)\n",
    "      dic = {}\n",
    "      for col_name in df.columns.values:\n",
    "        dic[col_name] = row[col_name]\n",
    "        \n",
    "      return ee.Feature(geometry, dic)\n",
    "\n",
    "  features = coor_points_df.apply(createFeature, axis=1).tolist()\n",
    "  fc = ee.FeatureCollection(features)\n",
    "\n",
    "\n",
    "  # Overlay the points on the imagery to get training.\n",
    "  overlayed_fc = img.sampleRegions(\n",
    "    collection= fc,\n",
    "    scale= 10 # maybe we should make this 10 instead\n",
    "  )\n",
    "  return overlayed_fc\n",
    "\n",
    "\n",
    "def export_to_drive(fc: ee.FeatureCollection, file:str, folder:str=\"NASA_Harvest\"):\n",
    "\n",
    "  # Export the ee.FeatureCollection as a .GeoJSON file.\n",
    "  task = ee.batch.Export.table.toDrive(**{\n",
    "    'collection': fc,\n",
    "    'description':file,\n",
    "    'fileFormat': 'GeoJSON',\n",
    "    'folder': folder\n",
    "  })\n",
    "  task.start()\n",
    "\n",
    "\n",
    "  print('----')\n",
    "  print(f'Polling for file name= {file}...')\n",
    "  while task.active():\n",
    "    time.sleep(5)\n",
    "  print(f'Wrote {file}.GeoJSON. Check {folder} folder in Drive.')\n",
    "  print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def read_geojson(file:str, date_col_name='finHarvDat') -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "        Expects the date column to have this format \"dd/mm/yy\", example: \"14/07/22\".\n",
    "        Expects that the file path looks like this: ../data/{file}.geojson\n",
    "    Args:\n",
    "        file (str): name of file (without the .geojson extention)\n",
    "        date_col_name (str, optional):\n",
    "        if df has no date column, pass None. Defaults to 'finHarvDat'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "        \n",
    "    Usage:\n",
    "            read_geojson(file='points_from_sph').head()\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def to_datetime(string):\n",
    "        if(string == 'nan' or not string):\n",
    "            return None\n",
    "        splitted = string.split('/')\n",
    "\n",
    "        day = int(splitted[0])\n",
    "        month = int(splitted[1])\n",
    "        year = 2000 + int(splitted[2])\n",
    "        datetime_object = datetime(year=year, month = month, day=day)#datetime.strptime(string, '%d-%m-%Y')\n",
    "        return datetime_object\n",
    "    \n",
    "    df = geopandas.read_file(f\"../data/{file}.geojson\")\n",
    "    def fixDate(x):\n",
    "        # 2013-06-22 to 13/06/22\t\n",
    "        string = str(x)\n",
    "        return string[2:4] + \"/\" + string[5:7] + \"/\" + string[8:]\n",
    "    \n",
    "    df[\"finHarvDat\"] = df[\"finHarvDat\"].apply(fixDate)\n",
    "    \n",
    "    if(date_col_name != None):\n",
    "        df[date_col_name] = df[date_col_name].apply(to_datetime)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3-week image collection from sentindel2 in 2022\n",
    "\n",
    "reference: https://medium.com/@moraesd90/creating-monthly-ndvi-composites-sentinel-2-on-google-earth-engine-a5c2d49bc9ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(s2_img_collection: ee.ImageCollection) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        ee.image: a cloud masked sentinel-2 image.\n",
    "    \"\"\"\n",
    "    def cloudmask_and_clip(image: ee.Image) -> ee.Image:\n",
    "        opaqueClouds_mask = 1 << 10\n",
    "        cirrusClouds_mask =1 << 11\n",
    "        bit_mask =opaqueClouds_mask | cirrusClouds_mask\n",
    "        qa = image.select('QA60')\n",
    "        mask = qa.bitwiseAnd(bit_mask).eq(0)\n",
    "        return image.clip(COUNTRY_GEOMETRY).updateMask(mask)\n",
    "    \n",
    "    def add_ndvi(image):\n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        image = image.addBands(ndvi.toFloat())\n",
    "        return image.toFloat()\n",
    "    \n",
    "    default_value = 0.0\n",
    "    image = s2_img_collection.map(cloudmask_and_clip).select(BANDS).filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 20)).median().unmask(default_value).float()\n",
    "    image = add_ndvi(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the new image collection: 17\n"
     ]
    }
   ],
   "source": [
    "DATE_START = ee.Date('2022-01-01')\n",
    "DATE_END= ee.Date('2022-12-27')\n",
    "SURF_REF_SEN2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\").filterDate(DATE_START, DATE_END)\n",
    "\n",
    "# start_weeks.getInfo -> [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52]\n",
    "total_weeks = ee.Number(DATE_END.difference(DATE_START, 'week')).round().getInfo()\n",
    "start_weeks = ee.List.sequence(1, total_weeks, 3)\n",
    "\n",
    "def extract_subset(start_week):\n",
    "    \n",
    "    start = DATE_START.advance(start_week, 'week')\n",
    "    end = start.advance(3, 'week').advance(-1, 'day')\n",
    "    \n",
    "    def getCollection():\n",
    "        return SURF_REF_SEN2.filterDate(start, end)\n",
    "    \n",
    "    img_collection = getCollection()\n",
    "    return get_image(img_collection)\n",
    "    \n",
    "    \n",
    "\n",
    "# Map the extract_subset function over the list of start weeks to create a new image collection that contains the subsets of the original image collection\n",
    "new_img_collection = ee.ImageCollection.fromImages(start_weeks.map(extract_subset))\n",
    "num_of_images = new_img_collection.size().getInfo()\n",
    "# Print the number of images in the new image collection\n",
    "print('Number of images in the new image collection:', num_of_images)\n",
    "images_list = new_img_collection.toList(num_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ee.ee_list.List at 0x17698ad40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ee.Image(images_list.get(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<folium.map.LayerControl at 0x17694a200>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_params = {\n",
    "  \"min\": 0,\n",
    "  \"max\": 3000,\n",
    "  \"bands\": [\"B4\", \"B3\", \"B2\"],\n",
    "}\n",
    "map = folium.Map(location=COUNTRY_LATLON, zoom_start=13)\n",
    "image=image.clip(COUNTRY_GEOMETRY)\n",
    "mapid = image.getMapId(vis_params)\n",
    "\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "folium.LayerControl().add_to(map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_16bf34d4000c70f22443b57674705d74 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_16bf34d4000c70f22443b57674705d74&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_16bf34d4000c70f22443b57674705d74 = L.map(\n",
       "                &quot;map_16bf34d4000c70f22443b57674705d74&quot;,\n",
       "                {\n",
       "                    center: [50.0, 31.0],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 13,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_dfe55ee01f421c7a7878c2eb9158561f = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_16bf34d4000c70f22443b57674705d74);\n",
       "        \n",
       "    \n",
       "            var tile_layer_59dfaec1615c92dfc3c6838984eb5f08 = L.tileLayer(\n",
       "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/c54cddc7f7a45e14618e05463b4996fd-be000cce58fa1f057d6bb20767d00afe/tiles/{z}/{x}/{y}&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_16bf34d4000c70f22443b57674705d74);\n",
       "        \n",
       "    \n",
       "            var layer_control_b8990c4dce0afd304fe8b0ee1db02f43 = {\n",
       "                base_layers : {\n",
       "                    &quot;openstreetmap&quot; : tile_layer_dfe55ee01f421c7a7878c2eb9158561f,\n",
       "                },\n",
       "                overlays :  {\n",
       "                    &quot;median composite&quot; : tile_layer_59dfaec1615c92dfc3c6838984eb5f08,\n",
       "                },\n",
       "            };\n",
       "            L.control.layers(\n",
       "                layer_control_b8990c4dce0afd304fe8b0ee1db02f43.base_layers,\n",
       "                layer_control_b8990c4dce0afd304fe8b0ee1db02f43.overlays,\n",
       "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
       "            ).addTo(map_16bf34d4000c70f22443b57674705d74);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x171846980>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_dates = []\n",
    "start_weeks = ee.List.sequence(1, total_weeks, 3)\n",
    "\n",
    "# record the dates into start_end_dates array\n",
    "def eeDate_to_datetime(eeDate: ee.Date)->datetime:\n",
    "    year = eeDate.get('year').getInfo()\n",
    "    month = eeDate.get('month').getInfo()\n",
    "    day = eeDate.get('day').getInfo()\n",
    "    return datetime(year=year, month=month, day=day)\n",
    "\n",
    "for idx in range(num_of_images):\n",
    "    start_week = start_weeks.get(idx).getInfo()\n",
    "    start = DATE_START.advance(start_week, 'week')\n",
    "    end = start.advance(3, 'week').advance(-1, 'day')\n",
    "    start_end_dates.append((eeDate_to_datetime(start), eeDate_to_datetime(end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((datetime.datetime(2022, 1, 8, 0, 0), datetime.datetime(2022, 1, 28, 0, 0)),\n",
       " 17)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_dates[0], len(start_end_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [f'img{idx}_overlayed' for idx in range(num_of_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "\n",
    "# for each image, overylay the points(aka export to drive)\n",
    "for idx in range(12, num_of_images):\n",
    "    currImg = ee.Image(images_list.get(idx))\n",
    "    file = f'img{idx}_overlayed'\n",
    "    export_to_drive(overlay_points(currImg, coor_points_df),file=file )    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download From Drive & Move to 'data' Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# true if (is_harvested is true) and if (the finHarvDat is within start_date & end_date)\n",
    "def get__is_within_period(row):\n",
    "    def date_within_range(dateToCheck:datetime, startDate:datetime, endDate:datetime):\n",
    "        \"\"\"credit: https://stackoverflow.com/users/22656/jon-skeet\"\"\"\n",
    "        return dateToCheck >= startDate and dateToCheck <= endDate\n",
    "    return row['is_harvested'] and date_within_range(row['finHarvDat'], row['start_date'], row['end_date'])\n",
    "    \n",
    "#samples = ee.List([]) # containing images\n",
    "samples = [None] * (num_of_images) # sample[0] is None\n",
    "\n",
    "path_inside_data_folder= OVERLAYED_FOLDER_NAMES[1]\n",
    "a= None\n",
    "for idx in range(num_of_images):\n",
    "    curr_img_df = read_geojson(file=path_inside_data_folder + file_names[idx])\n",
    "    \n",
    "    curr_img_df = curr_img_df.sort_values('point_id') # to make sure the points are aligned when we subtract ndvi values below\n",
    "    \n",
    "    # add time cols\n",
    "    start, end = start_end_dates[idx]\n",
    "    curr_img_df['start_date'] = np.tile(np.array([start]), curr_img_df.shape[0])\n",
    "    curr_img_df['end_date'] = np.tile(np.array([end]), curr_img_df.shape[0])\n",
    "    \n",
    "    curr_img_df['is_within_period'] = curr_img_df.apply(get__is_within_period, axis=1)\n",
    "    \n",
    "    curr_img_df['image_idx'] = np.tile(np.array(['i'+str(idx)]), curr_img_df.shape[0])\n",
    "    curr_img_df.rename(columns = {'point_id':'point_idx'}, inplace = True)\n",
    "    curr_img_df.point_idx = curr_img_df.point_idx.apply(lambda x: 'p' + str(x))\n",
    "    \n",
    "    samples[idx] = curr_img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49606, 19)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_images = pandas.concat(samples, sort=False)\n",
    "merged_images = merged_images.drop(['geometry', 'id', 'is_harvested'], axis=1)\n",
    "merged_images[BANDS] /= 10000 # # divide by 10000 bc the bands are scaled by 10000\n",
    "                                #(according to https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2)\n",
    "merged_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>finHarvDat</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>point_idx</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>is_within_period</th>\n",
       "      <th>image_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>46.347320</td>\n",
       "      <td>33.750930</td>\n",
       "      <td>p0</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2643</td>\n",
       "      <td>0.2107</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.1268</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.224248</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>46.278183</td>\n",
       "      <td>28.990823</td>\n",
       "      <td>p1</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>47.073863</td>\n",
       "      <td>33.093223</td>\n",
       "      <td>p2</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.2996</td>\n",
       "      <td>0.3321</td>\n",
       "      <td>0.3483</td>\n",
       "      <td>0.3758</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.3814</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>49.455416</td>\n",
       "      <td>38.596773</td>\n",
       "      <td>p3</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>48.101587</td>\n",
       "      <td>33.723420</td>\n",
       "      <td>p4</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>False</td>\n",
       "      <td>i0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B11     B12      B2      B3      B4      B5      B6      B7      B8   \n",
       "0  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  \\\n",
       "1  0.2643  0.2107  0.0356  0.0523  0.0915  0.1225  0.1268  0.1340  0.1444   \n",
       "2  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
       "3  0.1392  0.1225  0.2996  0.3321  0.3483  0.3758  0.3833  0.3814  0.3987   \n",
       "4  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
       "\n",
       "      B8A      NDVI finHarvDat        lat        lon point_idx start_date   \n",
       "0  0.0000  0.000000 2022-06-13  46.347320  33.750930        p0 2022-01-08  \\\n",
       "1  0.1570  0.224248 2022-06-13  46.278183  28.990823        p1 2022-01-08   \n",
       "2  0.0000  0.000000 2022-06-13  47.073863  33.093223        p2 2022-01-08   \n",
       "3  0.3801  0.067470 2022-06-13  49.455416  38.596773        p3 2022-01-08   \n",
       "4  0.0000  0.000000 2022-06-13  48.101587  33.723420        p4 2022-01-08   \n",
       "\n",
       "    end_date  is_within_period image_idx  \n",
       "0 2022-01-28             False        i0  \n",
       "1 2022-01-28             False        i0  \n",
       "2 2022-01-28             False        i0  \n",
       "3 2022-01-28             False        i0  \n",
       "4 2022-01-28             False        i0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(merged_images.is_within_period) # we expect 366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas df to geopandas df\n",
    "merged_samples_gdf = geopandas.GeoDataFrame(\n",
    "    merged_images, geometry=geopandas.points_from_xy(merged_images.lon, merged_images.lat))\n",
    "merged_samples_gdf.rename(columns = {'is_within_period':'har_evnt'}, inplace = True)\n",
    "merged_samples_gdf = merged_samples_gdf[(merged_samples_gdf.NDVI) != 0] # drop invalid points\n",
    "\n",
    "\n",
    "# save dataset\n",
    "merged_samples_gdf.to_file(f\"../data/{MERGED_FILE_NAME}\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('harvest-event-detec-Fyp2lGEN-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cf1faa60ab086dde8d867eb16c20b4bffc52bbf785c13cdda65e2052ce97abb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

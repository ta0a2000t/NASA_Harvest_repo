{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:47:49.081542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try only with best performing numeric columns for input into model first\n",
    "# TODO select features for deep learning model after ranking features with random forest (feature importance)\n",
    "# TODO lstm network\n",
    "# TODO SMOTE algorithm, for upsampling havested fields; we only have ~300 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local scripts\n",
    "from scripts import veg_indices, utilities, plots\n",
    "#from scripts.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "BANDS_DICT = {   'B2': 'Blue',\n",
    "            'B3': 'Green',\n",
    "            'B4': 'Red',\n",
    "            'B5': 'Red_Edge_1',\n",
    "            'B6': 'Red_Edge_2',\n",
    "            'B7': 'Red_Edge_3',\n",
    "            'B8': 'NIR',\n",
    "            'B8A': 'Red_Edge_4',\n",
    "            'B11': 'SWIR_1',\n",
    "            'B12': 'SWIR_2'}\n",
    "\n",
    "BANDS = list(BANDS_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "DF = geopandas.read_file('../data/merged_images.geojson')\n",
    "DF.rename(columns = {'is_within_period':'har_evnt'}, inplace = True)\n",
    "NUM_SAMPLES = len(np.unique(DF.image_idx)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added:  ['DVI', 'RVI', 'ARVI', 'PSSRa', 'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv', 'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI']\n",
      "Added:  ['sample_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff', 'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff', 'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff', 'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff', 'NDWI_diff', 'NDSI_diff', 'NDVI_diff']\n",
      "Index(['B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'NDVI',\n",
      "       'finHarvDat', 'lat', 'lon', 'point_idx', 'start_date', 'end_date',\n",
      "       'har_evnt', 'image_idx', 'geometry', 'DVI', 'RVI', 'ARVI', 'PSSRa',\n",
      "       'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv',\n",
      "       'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI',\n",
      "       'pt_idx', 'img_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff',\n",
      "       'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff',\n",
      "       'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff',\n",
      "       'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff',\n",
      "       'NDWI_diff', 'NDSI_diff', 'NDVI_diff', 'sample_idx'],\n",
      "      dtype='object') (2083, 66)\n"
     ]
    }
   ],
   "source": [
    "# cpied from learning_about-data.ipynb\n",
    "df = DF.copy()\n",
    "df = df[(df.NDVI) != 0] # drop invalid points\n",
    "VEG_INDICES_NAMES = veg_indices.add_veg_indices(df) + ['NDVI'] \n",
    "VEG_DIFF_NAMES = veg_indices.add_veg_diff(df, VEG_INDICES_NAMES)\n",
    "NUMERIC_COLS = BANDS + VEG_INDICES_NAMES + VEG_DIFF_NAMES\n",
    "df = df.dropna(subset=['sample_idx']) # removes the first image, because sample_idx is NaN there\n",
    "\n",
    "\n",
    "#\"\"\" \n",
    "# only look at samples 6, 7, 8, 9\n",
    "new_df = None\n",
    "for i in range(6, 9 + 1):\n",
    "\n",
    "    curr_df = df[df[\"sample_idx\"] == f\"s{i}\"]\n",
    "    if(type(new_df) == type(None)):\n",
    "        new_df = curr_df\n",
    "    else:\n",
    "        new_df = pd.concat([new_df, curr_df])\n",
    "df = new_df\n",
    "#\"\"\"\n",
    "\n",
    "print(df.columns, df.shape)\n",
    "\n",
    "\n",
    "# For each 3-week image, standarize each column\n",
    "df = utilities.get_rm_outlier_standarize(df, NUMERIC_COLS, rm_outliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(df:pd.DataFrame, colName: str, retArrays=False):\n",
    "    lookupTable, indexed_dataSet = np.unique(df[colName], return_inverse=True)\n",
    "    tpls = tf.keras.utils.to_categorical(indexed_dataSet)\n",
    "\n",
    "    arrays = tpls.T[1:] # remove one column (not needed)\n",
    "    arrays = arrays.astype(np.float32) # tensorflow does not support bools, so float 32 is the best we can do :/\n",
    "    if(retArrays):\n",
    "        return arrays\n",
    "\n",
    "    return arrays.T # return to n by m (n tuples of size m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>...</th>\n",
       "      <th>NDTI_diff</th>\n",
       "      <th>NDMI_diff</th>\n",
       "      <th>MSI_diff</th>\n",
       "      <th>GCI_diff</th>\n",
       "      <th>NBRI_diff</th>\n",
       "      <th>BSI_diff</th>\n",
       "      <th>NDWI_diff</th>\n",
       "      <th>NDSI_diff</th>\n",
       "      <th>NDVI_diff</th>\n",
       "      <th>sample_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>-0.870086</td>\n",
       "      <td>-0.894463</td>\n",
       "      <td>-0.362898</td>\n",
       "      <td>-0.562899</td>\n",
       "      <td>-0.712486</td>\n",
       "      <td>-0.803480</td>\n",
       "      <td>-0.029281</td>\n",
       "      <td>0.751010</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.729749</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.478007</td>\n",
       "      <td>1.603634</td>\n",
       "      <td>-1.360793</td>\n",
       "      <td>2.700617</td>\n",
       "      <td>-2.968218</td>\n",
       "      <td>-1.523347</td>\n",
       "      <td>0.521851</td>\n",
       "      <td>1.005915</td>\n",
       "      <td>1.573112</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>-0.312600</td>\n",
       "      <td>-0.369832</td>\n",
       "      <td>-0.086144</td>\n",
       "      <td>0.280409</td>\n",
       "      <td>-0.099855</td>\n",
       "      <td>0.208097</td>\n",
       "      <td>0.629703</td>\n",
       "      <td>0.362093</td>\n",
       "      <td>0.061773</td>\n",
       "      <td>0.152029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.370216</td>\n",
       "      <td>-0.424915</td>\n",
       "      <td>-0.590995</td>\n",
       "      <td>0.906889</td>\n",
       "      <td>-0.333211</td>\n",
       "      <td>1.112289</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>-0.610115</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>0.709577</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-0.152589</td>\n",
       "      <td>-0.109988</td>\n",
       "      <td>-0.493141</td>\n",
       "      <td>-0.196534</td>\n",
       "      <td>1.214141</td>\n",
       "      <td>1.359916</td>\n",
       "      <td>1.234850</td>\n",
       "      <td>1.216424</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.587707</td>\n",
       "      <td>-0.081406</td>\n",
       "      <td>-0.394088</td>\n",
       "      <td>0.704432</td>\n",
       "      <td>-0.719594</td>\n",
       "      <td>-0.579386</td>\n",
       "      <td>-0.135283</td>\n",
       "      <td>0.200484</td>\n",
       "      <td>1.265219</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>-0.561720</td>\n",
       "      <td>-0.722576</td>\n",
       "      <td>-0.312302</td>\n",
       "      <td>-0.473593</td>\n",
       "      <td>-0.729585</td>\n",
       "      <td>-0.671262</td>\n",
       "      <td>0.668467</td>\n",
       "      <td>1.463120</td>\n",
       "      <td>2.055841</td>\n",
       "      <td>1.519354</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.537627</td>\n",
       "      <td>1.919438</td>\n",
       "      <td>-1.531427</td>\n",
       "      <td>3.167865</td>\n",
       "      <td>-3.512676</td>\n",
       "      <td>-1.735957</td>\n",
       "      <td>0.806501</td>\n",
       "      <td>1.165113</td>\n",
       "      <td>1.692973</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>-0.299037</td>\n",
       "      <td>-0.194955</td>\n",
       "      <td>-0.324494</td>\n",
       "      <td>-0.356218</td>\n",
       "      <td>-0.293845</td>\n",
       "      <td>-0.373203</td>\n",
       "      <td>-0.754462</td>\n",
       "      <td>-0.892654</td>\n",
       "      <td>-0.904964</td>\n",
       "      <td>-0.916228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557096</td>\n",
       "      <td>-0.683240</td>\n",
       "      <td>0.500220</td>\n",
       "      <td>-0.312166</td>\n",
       "      <td>0.231992</td>\n",
       "      <td>0.551830</td>\n",
       "      <td>-0.896480</td>\n",
       "      <td>-0.580907</td>\n",
       "      <td>0.203664</td>\n",
       "      <td>s6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>-1.223664</td>\n",
       "      <td>-1.653289</td>\n",
       "      <td>-0.874174</td>\n",
       "      <td>-0.848826</td>\n",
       "      <td>-1.376933</td>\n",
       "      <td>-0.955374</td>\n",
       "      <td>1.352084</td>\n",
       "      <td>1.767256</td>\n",
       "      <td>1.749997</td>\n",
       "      <td>1.694362</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.565219</td>\n",
       "      <td>1.862842</td>\n",
       "      <td>-1.964314</td>\n",
       "      <td>2.941633</td>\n",
       "      <td>-3.306606</td>\n",
       "      <td>-2.374764</td>\n",
       "      <td>0.411842</td>\n",
       "      <td>2.136983</td>\n",
       "      <td>2.313166</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>0.585052</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>-0.201453</td>\n",
       "      <td>-0.167603</td>\n",
       "      <td>0.086507</td>\n",
       "      <td>-0.184328</td>\n",
       "      <td>-0.739681</td>\n",
       "      <td>-0.749316</td>\n",
       "      <td>-0.777399</td>\n",
       "      <td>-0.679339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142589</td>\n",
       "      <td>-0.838315</td>\n",
       "      <td>1.168225</td>\n",
       "      <td>-0.907196</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>1.137787</td>\n",
       "      <td>-0.727334</td>\n",
       "      <td>-1.122512</td>\n",
       "      <td>-0.399597</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135</th>\n",
       "      <td>-0.186510</td>\n",
       "      <td>0.046045</td>\n",
       "      <td>-0.259642</td>\n",
       "      <td>-0.345125</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>-0.173940</td>\n",
       "      <td>-0.687242</td>\n",
       "      <td>-0.689586</td>\n",
       "      <td>-0.657617</td>\n",
       "      <td>-0.606040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554367</td>\n",
       "      <td>-0.367829</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>-0.382788</td>\n",
       "      <td>0.256747</td>\n",
       "      <td>0.560347</td>\n",
       "      <td>-0.615972</td>\n",
       "      <td>-0.461928</td>\n",
       "      <td>-0.459774</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6136</th>\n",
       "      <td>0.421999</td>\n",
       "      <td>1.102268</td>\n",
       "      <td>-0.817404</td>\n",
       "      <td>-1.045111</td>\n",
       "      <td>-0.845186</td>\n",
       "      <td>-1.091576</td>\n",
       "      <td>-1.444705</td>\n",
       "      <td>-1.206955</td>\n",
       "      <td>-1.128188</td>\n",
       "      <td>-1.259767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706420</td>\n",
       "      <td>-1.028375</td>\n",
       "      <td>1.619866</td>\n",
       "      <td>-0.555706</td>\n",
       "      <td>-0.749974</td>\n",
       "      <td>1.448490</td>\n",
       "      <td>-2.587140</td>\n",
       "      <td>-1.076003</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>-0.608432</td>\n",
       "      <td>-0.292357</td>\n",
       "      <td>-0.669803</td>\n",
       "      <td>-0.638109</td>\n",
       "      <td>-0.802010</td>\n",
       "      <td>-0.760304</td>\n",
       "      <td>-0.322105</td>\n",
       "      <td>-0.291678</td>\n",
       "      <td>-0.341051</td>\n",
       "      <td>-0.349493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766941</td>\n",
       "      <td>0.605088</td>\n",
       "      <td>-0.278171</td>\n",
       "      <td>0.380161</td>\n",
       "      <td>-0.491515</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.655980</td>\n",
       "      <td>0.251211</td>\n",
       "      <td>0.849334</td>\n",
       "      <td>s9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2083 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           B11       B12        B2        B3        B4        B5        B6  \\\n",
       "3906 -0.870086 -0.894463 -0.362898 -0.562899 -0.712486 -0.803480 -0.029281   \n",
       "3907 -0.312600 -0.369832 -0.086144  0.280409 -0.099855  0.208097  0.629703   \n",
       "3908  0.709577  0.053909 -0.152589 -0.109988 -0.493141 -0.196534  1.214141   \n",
       "3909 -0.561720 -0.722576 -0.312302 -0.473593 -0.729585 -0.671262  0.668467   \n",
       "3910 -0.299037 -0.194955 -0.324494 -0.356218 -0.293845 -0.373203 -0.754462   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6133 -1.223664 -1.653289 -0.874174 -0.848826 -1.376933 -0.955374  1.352084   \n",
       "6134  0.585052  0.995327 -0.201453 -0.167603  0.086507 -0.184328 -0.739681   \n",
       "6135 -0.186510  0.046045 -0.259642 -0.345125  0.100141 -0.173940 -0.687242   \n",
       "6136  0.421999  1.102268 -0.817404 -1.045111 -0.845186 -1.091576 -1.444705   \n",
       "6137 -0.608432 -0.292357 -0.669803 -0.638109 -0.802010 -0.760304 -0.322105   \n",
       "\n",
       "            B7        B8       B8A  ...  NDTI_diff NDMI_diff  MSI_diff  \\\n",
       "3906  0.751010  0.946631  0.729749  ...  -1.478007  1.603634 -1.360793   \n",
       "3907  0.362093  0.061773  0.152029  ...   0.003610  0.370216 -0.424915   \n",
       "3908  1.359916  1.234850  1.216424  ...  -1.587707 -0.081406 -0.394088   \n",
       "3909  1.463120  2.055841  1.519354  ...  -2.537627  1.919438 -1.531427   \n",
       "3910 -0.892654 -0.904964 -0.916228  ...   0.557096 -0.683240  0.500220   \n",
       "...        ...       ...       ...  ...        ...       ...       ...   \n",
       "6133  1.767256  1.749997  1.694362  ...  -1.565219  1.862842 -1.964314   \n",
       "6134 -0.749316 -0.777399 -0.679339  ...   0.142589 -0.838315  1.168225   \n",
       "6135 -0.689586 -0.657617 -0.606040  ...   0.554367 -0.367829  0.398082   \n",
       "6136 -1.206955 -1.128188 -1.259767  ...  -0.706420 -1.028375  1.619866   \n",
       "6137 -0.291678 -0.341051 -0.349493  ...  -0.766941  0.605088 -0.278171   \n",
       "\n",
       "      GCI_diff NBRI_diff  BSI_diff NDWI_diff  NDSI_diff NDVI_diff sample_idx  \n",
       "3906  2.700617 -2.968218 -1.523347  0.521851   1.005915  1.573112         s6  \n",
       "3907 -0.590995  0.906889 -0.333211  1.112289   0.629439 -0.610115         s6  \n",
       "3908  0.704432 -0.719594 -0.579386 -0.135283   0.200484  1.265219         s6  \n",
       "3909  3.167865 -3.512676 -1.735957  0.806501   1.165113  1.692973         s6  \n",
       "3910 -0.312166  0.231992  0.551830 -0.896480  -0.580907  0.203664         s6  \n",
       "...        ...       ...       ...       ...        ...       ...        ...  \n",
       "6133  2.941633 -3.306606 -2.374764  0.411842   2.136983  2.313166         s9  \n",
       "6134 -0.907196  0.397156  1.137787 -0.727334  -1.122512 -0.399597         s9  \n",
       "6135 -0.382788  0.256747  0.560347 -0.615972  -0.461928 -0.459774         s9  \n",
       "6136 -0.555706 -0.749974  1.448490 -2.587140  -1.076003  0.980113         s9  \n",
       "6137  0.380161 -0.491515 -0.017271 -0.655980   0.251211  0.849334         s9  \n",
       "\n",
       "[2083 rows x 66 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'NDVI',\n",
      "       'finHarvDat', 'lat', 'lon', 'point_idx', 'start_date', 'end_date',\n",
      "       'har_evnt', 'image_idx', 'geometry', 'DVI', 'RVI', 'ARVI', 'PSSRa',\n",
      "       'NDI45', 'GNDVI', 'MCARI', 'IRECI', 'CIr', 'MTCI', 'NDVIre', 'NIRv',\n",
      "       'EVI', 'NDTI', 'NDMI', 'MSI', 'GCI', 'NBRI', 'BSI', 'NDWI', 'NDSI',\n",
      "       'pt_idx', 'img_idx', 'DVI_diff', 'RVI_diff', 'ARVI_diff', 'PSSRa_diff',\n",
      "       'NDI45_diff', 'GNDVI_diff', 'MCARI_diff', 'IRECI_diff', 'CIr_diff',\n",
      "       'MTCI_diff', 'NDVIre_diff', 'NIRv_diff', 'EVI_diff', 'NDTI_diff',\n",
      "       'NDMI_diff', 'MSI_diff', 'GCI_diff', 'NBRI_diff', 'BSI_diff',\n",
      "       'NDWI_diff', 'NDSI_diff', 'NDVI_diff', 'sample_idx',\n",
      "       'sample_idx_1hot_0', 'sample_idx_1hot_1', 'sample_idx_1hot_2',\n",
      "       'har_evnt_1hot_0'],\n",
      "      dtype='object')\n",
      "['sample_idx_1hot_0', 'sample_idx_1hot_1', 'sample_idx_1hot_2']\n",
      "['har_evnt_1hot_0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_to_1hot(categorical_col_name:str):\n",
    "    names = []\n",
    "    arrays = get_one_hot(df, categorical_col_name, retArrays=True)\n",
    "    for i in range(len(arrays)):\n",
    "        arr = arrays[i]\n",
    "        name = f'{categorical_col_name}_1hot_{i}'\n",
    "        df[name] = arr\n",
    "        names.append(name)\n",
    "    return names\n",
    "\n",
    "one_hot_names_X = convert_to_1hot(\"sample_idx\")\n",
    "one_hot_names_Y = convert_to_1hot(\"har_evnt\")\n",
    "\n",
    "print(df.columns)\n",
    "print(one_hot_names_X)\n",
    "print(one_hot_names_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reshape the data into a 3D tensor with shape (num_samples, sequence_length, num_features) to feed it into a 1D CNN in TensorFlow for training.\n",
    "# sequence_length: each 3-week sample\n",
    "# num_samples: num of points, aka farms\n",
    "\n",
    "points_dfs_X = []\n",
    "points_dfs_Y = []\n",
    "\n",
    "for p in sorted(df['point_idx'].unique(), key= lambda x: int(x[1:])):\n",
    "    curr_df = df[df[\"point_idx\"] == p]\n",
    "\n",
    "    if(curr_df.shape[0] == 4): #  TODO fix this, even method1 is not done correctly :/ \n",
    "        \"\"\"\n",
    "        TODO\n",
    "        use one of the follwoing(currently using method 1):\n",
    "            1-Remove the rows with missing values.\n",
    "            2-Fill in missing values with a constant\n",
    "            3-Interpolate missing values\n",
    "            4-Use imputation methods\n",
    "        \"\"\"\n",
    "        curr_df_X = (curr_df[one_hot_names_X + BANDS]).to_numpy()\n",
    "        points_dfs_X.append(curr_df_X) \n",
    "\n",
    "        curr_df_Y = (curr_df[one_hot_names_Y]).to_numpy()\n",
    "        points_dfs_Y.append(curr_df_Y) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(points_dfs_X)\n",
    "Y = np.array(points_dfs_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430, 4, 13)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430, 4, 1)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.reshape((430, 4))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(4, 13)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=4)\n",
    "])\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 2s 28ms/step - loss: 0.1411 - accuracy: 0.3566 - val_loss: 0.1068 - val_accuracy: 0.3023\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1111 - accuracy: 0.4289 - val_loss: 0.1056 - val_accuracy: 0.3488\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.1055 - accuracy: 0.4315 - val_loss: 0.1053 - val_accuracy: 0.3256\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0978 - accuracy: 0.4496 - val_loss: 0.1055 - val_accuracy: 0.3256\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0928 - accuracy: 0.4574 - val_loss: 0.0984 - val_accuracy: 0.3256\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0890 - accuracy: 0.4987 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.4935 - val_loss: 0.0943 - val_accuracy: 0.3256\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0861 - accuracy: 0.4935 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.0795 - accuracy: 0.5013 - val_loss: 0.0878 - val_accuracy: 0.4186\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0768 - accuracy: 0.5349 - val_loss: 0.0811 - val_accuracy: 0.3488\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.5297 - val_loss: 0.0900 - val_accuracy: 0.3721\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0752 - accuracy: 0.5711 - val_loss: 0.0843 - val_accuracy: 0.3488\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0701 - accuracy: 0.5426 - val_loss: 0.0838 - val_accuracy: 0.4186\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.5633 - val_loss: 0.0831 - val_accuracy: 0.3488\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0640 - accuracy: 0.5711 - val_loss: 0.0855 - val_accuracy: 0.3721\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0633 - accuracy: 0.5995 - val_loss: 0.0821 - val_accuracy: 0.3488\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0631 - accuracy: 0.5762 - val_loss: 0.0814 - val_accuracy: 0.3721\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.5659 - val_loss: 0.0882 - val_accuracy: 0.3721\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0617 - accuracy: 0.6021 - val_loss: 0.0839 - val_accuracy: 0.4186\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.5995 - val_loss: 0.0924 - val_accuracy: 0.3953\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0513 - accuracy: 0.6176 - val_loss: 0.0950 - val_accuracy: 0.3721\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0493 - accuracy: 0.6253 - val_loss: 0.0971 - val_accuracy: 0.3721\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.5814 - val_loss: 0.0917 - val_accuracy: 0.3953\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.0579 - accuracy: 0.5891 - val_loss: 0.0923 - val_accuracy: 0.3953\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0507 - accuracy: 0.6021 - val_loss: 0.0925 - val_accuracy: 0.3953\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0522 - accuracy: 0.5866 - val_loss: 0.0974 - val_accuracy: 0.3488\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0495 - accuracy: 0.6150 - val_loss: 0.1028 - val_accuracy: 0.3721\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.6150 - val_loss: 0.0880 - val_accuracy: 0.3721\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0459 - accuracy: 0.6176 - val_loss: 0.0832 - val_accuracy: 0.4186\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0521 - accuracy: 0.6072 - val_loss: 0.1020 - val_accuracy: 0.3256\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.6305 - val_loss: 0.0924 - val_accuracy: 0.3953\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.6253 - val_loss: 0.0989 - val_accuracy: 0.3953\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0394 - accuracy: 0.6718 - val_loss: 0.0969 - val_accuracy: 0.3721\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.6279 - val_loss: 0.1047 - val_accuracy: 0.3953\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0480 - accuracy: 0.6382 - val_loss: 0.0979 - val_accuracy: 0.4651\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.6512 - val_loss: 0.0985 - val_accuracy: 0.4186\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0405 - accuracy: 0.6563 - val_loss: 0.0998 - val_accuracy: 0.4186\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0396 - accuracy: 0.6279 - val_loss: 0.0983 - val_accuracy: 0.4186\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0389 - accuracy: 0.6718 - val_loss: 0.0954 - val_accuracy: 0.4186\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0347 - accuracy: 0.6848 - val_loss: 0.1008 - val_accuracy: 0.3953\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.0348 - accuracy: 0.6589 - val_loss: 0.0967 - val_accuracy: 0.4186\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0339 - accuracy: 0.6822 - val_loss: 0.0992 - val_accuracy: 0.4186\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0301 - accuracy: 0.6641 - val_loss: 0.0933 - val_accuracy: 0.4186\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0290 - accuracy: 0.6796 - val_loss: 0.1072 - val_accuracy: 0.3953\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.6848 - val_loss: 0.0953 - val_accuracy: 0.4186\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0388 - accuracy: 0.6563 - val_loss: 0.1032 - val_accuracy: 0.3488\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0342 - accuracy: 0.6589 - val_loss: 0.1107 - val_accuracy: 0.3953\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.6486 - val_loss: 0.1053 - val_accuracy: 0.3721\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0383 - accuracy: 0.6357 - val_loss: 0.0917 - val_accuracy: 0.3721\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0329 - accuracy: 0.6822 - val_loss: 0.1087 - val_accuracy: 0.3721\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.6382 - val_loss: 0.1183 - val_accuracy: 0.3721\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0468 - accuracy: 0.6176 - val_loss: 0.0917 - val_accuracy: 0.3953\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0413 - accuracy: 0.6537 - val_loss: 0.1181 - val_accuracy: 0.3953\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0354 - accuracy: 0.6537 - val_loss: 0.1002 - val_accuracy: 0.3953\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.6537 - val_loss: 0.1245 - val_accuracy: 0.3721\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0298 - accuracy: 0.6718 - val_loss: 0.1007 - val_accuracy: 0.3953\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0306 - accuracy: 0.6615 - val_loss: 0.1095 - val_accuracy: 0.3721\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0312 - accuracy: 0.6641 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0282 - accuracy: 0.6563 - val_loss: 0.1224 - val_accuracy: 0.3953\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0286 - accuracy: 0.6589 - val_loss: 0.0969 - val_accuracy: 0.3721\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 0.6667 - val_loss: 0.1039 - val_accuracy: 0.4186\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 0.6615 - val_loss: 0.1144 - val_accuracy: 0.3721\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.6796 - val_loss: 0.1074 - val_accuracy: 0.3953\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0220 - accuracy: 0.6770 - val_loss: 0.1211 - val_accuracy: 0.3256\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0314 - accuracy: 0.6357 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0312 - accuracy: 0.6512 - val_loss: 0.1203 - val_accuracy: 0.3953\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0246 - accuracy: 0.6641 - val_loss: 0.1104 - val_accuracy: 0.3488\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0263 - accuracy: 0.6589 - val_loss: 0.1162 - val_accuracy: 0.3488\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0290 - accuracy: 0.6434 - val_loss: 0.1241 - val_accuracy: 0.3721\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0318 - accuracy: 0.6460 - val_loss: 0.1101 - val_accuracy: 0.3953\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0313 - accuracy: 0.6667 - val_loss: 0.1081 - val_accuracy: 0.3488\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0253 - accuracy: 0.6693 - val_loss: 0.1128 - val_accuracy: 0.3953\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0278 - accuracy: 0.6693 - val_loss: 0.1105 - val_accuracy: 0.3488\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0227 - accuracy: 0.6693 - val_loss: 0.0970 - val_accuracy: 0.3488\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0248 - accuracy: 0.6667 - val_loss: 0.1065 - val_accuracy: 0.3488\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0242 - accuracy: 0.6822 - val_loss: 0.1056 - val_accuracy: 0.3721\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0215 - accuracy: 0.6744 - val_loss: 0.0998 - val_accuracy: 0.3953\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 0.6434 - val_loss: 0.0929 - val_accuracy: 0.3953\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 0.6615 - val_loss: 0.0964 - val_accuracy: 0.4186\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.6796 - val_loss: 0.0934 - val_accuracy: 0.3721\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0286 - accuracy: 0.6667 - val_loss: 0.0968 - val_accuracy: 0.4186\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0253 - accuracy: 0.6641 - val_loss: 0.0993 - val_accuracy: 0.3721\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0264 - accuracy: 0.6718 - val_loss: 0.0979 - val_accuracy: 0.4186\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.6331 - val_loss: 0.1096 - val_accuracy: 0.4186\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0341 - accuracy: 0.6434 - val_loss: 0.1029 - val_accuracy: 0.4419\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.6434 - val_loss: 0.0921 - val_accuracy: 0.4186\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0257 - accuracy: 0.6537 - val_loss: 0.0961 - val_accuracy: 0.4186\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0226 - accuracy: 0.6615 - val_loss: 0.1049 - val_accuracy: 0.3953\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0210 - accuracy: 0.6693 - val_loss: 0.1056 - val_accuracy: 0.3953\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0229 - accuracy: 0.6615 - val_loss: 0.1156 - val_accuracy: 0.3953\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.0265 - accuracy: 0.6641 - val_loss: 0.0991 - val_accuracy: 0.3953\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0220 - accuracy: 0.6718 - val_loss: 0.1106 - val_accuracy: 0.3953\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0190 - accuracy: 0.6822 - val_loss: 0.0956 - val_accuracy: 0.4186\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0205 - accuracy: 0.6873 - val_loss: 0.1072 - val_accuracy: 0.3721\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.6770 - val_loss: 0.1021 - val_accuracy: 0.3953\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0238 - accuracy: 0.6718 - val_loss: 0.1040 - val_accuracy: 0.3488\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0252 - accuracy: 0.6744 - val_loss: 0.1098 - val_accuracy: 0.3721\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.6822 - val_loss: 0.1128 - val_accuracy: 0.3721\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0193 - accuracy: 0.6977 - val_loss: 0.1139 - val_accuracy: 0.3953\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0213 - accuracy: 0.6770 - val_loss: 0.1084 - val_accuracy: 0.3953\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0183 - accuracy: 0.6822 - val_loss: 0.1085 - val_accuracy: 0.3721\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0152 - accuracy: 0.6951 - val_loss: 0.0906 - val_accuracy: 0.3721\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 0.6899 - val_loss: 0.1044 - val_accuracy: 0.3721\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0168 - accuracy: 0.6693 - val_loss: 0.1056 - val_accuracy: 0.3721\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.6589 - val_loss: 0.1068 - val_accuracy: 0.4186\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1115 - val_accuracy: 0.3721\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0270 - accuracy: 0.6563 - val_loss: 0.0903 - val_accuracy: 0.4186\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0224 - accuracy: 0.6615 - val_loss: 0.0975 - val_accuracy: 0.3953\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1005 - val_accuracy: 0.3953\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0210 - accuracy: 0.6667 - val_loss: 0.1023 - val_accuracy: 0.3721\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0221 - accuracy: 0.6641 - val_loss: 0.1012 - val_accuracy: 0.3721\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0183 - accuracy: 0.6744 - val_loss: 0.1073 - val_accuracy: 0.3953\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0202 - accuracy: 0.6822 - val_loss: 0.1000 - val_accuracy: 0.3953\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0171 - accuracy: 0.6873 - val_loss: 0.0926 - val_accuracy: 0.3721\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0192 - accuracy: 0.6925 - val_loss: 0.0979 - val_accuracy: 0.3721\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0217 - accuracy: 0.6615 - val_loss: 0.0911 - val_accuracy: 0.3953\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0168 - accuracy: 0.7158 - val_loss: 0.0870 - val_accuracy: 0.3953\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0214 - accuracy: 0.6977 - val_loss: 0.0858 - val_accuracy: 0.3721\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 0.6770 - val_loss: 0.1159 - val_accuracy: 0.3256\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.6641 - val_loss: 0.0966 - val_accuracy: 0.4186\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0151 - accuracy: 0.6796 - val_loss: 0.0949 - val_accuracy: 0.3953\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0166 - accuracy: 0.6744 - val_loss: 0.1064 - val_accuracy: 0.3953\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 0.6641 - val_loss: 0.1025 - val_accuracy: 0.3953\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0136 - accuracy: 0.6925 - val_loss: 0.1081 - val_accuracy: 0.3721\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0135 - accuracy: 0.6770 - val_loss: 0.1109 - val_accuracy: 0.3721\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0131 - accuracy: 0.6873 - val_loss: 0.1076 - val_accuracy: 0.3721\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 0.6744 - val_loss: 0.1080 - val_accuracy: 0.3488\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.6873 - val_loss: 0.1058 - val_accuracy: 0.3488\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0130 - accuracy: 0.6873 - val_loss: 0.0960 - val_accuracy: 0.3953\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.7106 - val_loss: 0.0902 - val_accuracy: 0.4186\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0128 - accuracy: 0.6951 - val_loss: 0.0959 - val_accuracy: 0.4186\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0114 - accuracy: 0.7003 - val_loss: 0.0991 - val_accuracy: 0.4186\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0130 - accuracy: 0.7442 - val_loss: 0.0982 - val_accuracy: 0.3721\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0168 - accuracy: 0.7158 - val_loss: 0.0983 - val_accuracy: 0.4186\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0194 - accuracy: 0.7287 - val_loss: 0.1103 - val_accuracy: 0.4186\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.0186 - accuracy: 0.6744 - val_loss: 0.1014 - val_accuracy: 0.3953\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 0.6925 - val_loss: 0.1151 - val_accuracy: 0.3488\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0244 - accuracy: 0.6537 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0231 - accuracy: 0.6848 - val_loss: 0.1113 - val_accuracy: 0.3488\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.6796 - val_loss: 0.1017 - val_accuracy: 0.3953\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 0.6925 - val_loss: 0.1142 - val_accuracy: 0.3488\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.7132 - val_loss: 0.1087 - val_accuracy: 0.3721\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0143 - accuracy: 0.6848 - val_loss: 0.1020 - val_accuracy: 0.4186\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0192 - accuracy: 0.6899 - val_loss: 0.1054 - val_accuracy: 0.3721\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0176 - accuracy: 0.6744 - val_loss: 0.1077 - val_accuracy: 0.3488\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 0.6615 - val_loss: 0.0982 - val_accuracy: 0.3721\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0197 - accuracy: 0.6693 - val_loss: 0.1045 - val_accuracy: 0.3488\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0160 - accuracy: 0.6693 - val_loss: 0.0901 - val_accuracy: 0.3953\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0151 - accuracy: 0.6899 - val_loss: 0.0989 - val_accuracy: 0.3953\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.6977 - val_loss: 0.1064 - val_accuracy: 0.3721\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.6770 - val_loss: 0.0989 - val_accuracy: 0.3721\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0142 - accuracy: 0.6770 - val_loss: 0.0984 - val_accuracy: 0.3488\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0192 - accuracy: 0.6822 - val_loss: 0.1019 - val_accuracy: 0.3721\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0203 - accuracy: 0.6718 - val_loss: 0.0936 - val_accuracy: 0.3953\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0177 - accuracy: 0.6770 - val_loss: 0.1021 - val_accuracy: 0.3953\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 0.6718 - val_loss: 0.1054 - val_accuracy: 0.3721\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0223 - accuracy: 0.6641 - val_loss: 0.0994 - val_accuracy: 0.3721\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0201 - accuracy: 0.6770 - val_loss: 0.1009 - val_accuracy: 0.3721\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0244 - accuracy: 0.6537 - val_loss: 0.1031 - val_accuracy: 0.3721\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.6693 - val_loss: 0.1004 - val_accuracy: 0.3721\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.0203 - accuracy: 0.6693 - val_loss: 0.1058 - val_accuracy: 0.3721\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.6641 - val_loss: 0.1058 - val_accuracy: 0.3488\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0150 - accuracy: 0.6848 - val_loss: 0.1064 - val_accuracy: 0.3953\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0114 - accuracy: 0.6951 - val_loss: 0.1135 - val_accuracy: 0.3488\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6977 - val_loss: 0.1092 - val_accuracy: 0.3488\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0119 - accuracy: 0.6899 - val_loss: 0.1078 - val_accuracy: 0.3721\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0120 - accuracy: 0.7028 - val_loss: 0.0966 - val_accuracy: 0.3488\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.6770 - val_loss: 0.1033 - val_accuracy: 0.3488\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 0.6770 - val_loss: 0.1150 - val_accuracy: 0.3488\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.6899 - val_loss: 0.1035 - val_accuracy: 0.3721\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0122 - accuracy: 0.6925 - val_loss: 0.1054 - val_accuracy: 0.3488\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 0.6873 - val_loss: 0.1127 - val_accuracy: 0.3488\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0138 - accuracy: 0.6770 - val_loss: 0.1108 - val_accuracy: 0.3721\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0116 - accuracy: 0.6822 - val_loss: 0.0975 - val_accuracy: 0.3721\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.6796 - val_loss: 0.0930 - val_accuracy: 0.4186\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6770 - val_loss: 0.0962 - val_accuracy: 0.3953\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 0.6977 - val_loss: 0.1032 - val_accuracy: 0.3953\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.6796 - val_loss: 0.0945 - val_accuracy: 0.3953\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.6848 - val_loss: 0.0936 - val_accuracy: 0.3953\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0129 - accuracy: 0.6873 - val_loss: 0.0956 - val_accuracy: 0.3488\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 0.6925 - val_loss: 0.0964 - val_accuracy: 0.3721\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0108 - accuracy: 0.6796 - val_loss: 0.1047 - val_accuracy: 0.3488\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0131 - accuracy: 0.6873 - val_loss: 0.0979 - val_accuracy: 0.3721\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0116 - accuracy: 0.6744 - val_loss: 0.1026 - val_accuracy: 0.3721\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.6693 - val_loss: 0.0938 - val_accuracy: 0.3721\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0092 - accuracy: 0.6977 - val_loss: 0.0986 - val_accuracy: 0.3488\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0155 - accuracy: 0.6770 - val_loss: 0.0967 - val_accuracy: 0.3488\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 0.6822 - val_loss: 0.0970 - val_accuracy: 0.3488\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 0.6822 - val_loss: 0.0941 - val_accuracy: 0.3721\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0113 - accuracy: 0.6848 - val_loss: 0.1010 - val_accuracy: 0.3488\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0101 - accuracy: 0.6951 - val_loss: 0.0916 - val_accuracy: 0.3721\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0102 - accuracy: 0.6873 - val_loss: 0.0897 - val_accuracy: 0.3488\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 0.6718 - val_loss: 0.0957 - val_accuracy: 0.3488\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0122 - accuracy: 0.6848 - val_loss: 0.0982 - val_accuracy: 0.3488\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.6899 - val_loss: 0.1066 - val_accuracy: 0.3488\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.6873 - val_loss: 0.0988 - val_accuracy: 0.3721\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0119 - accuracy: 0.6848 - val_loss: 0.1081 - val_accuracy: 0.3721\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.6848 - val_loss: 0.1109 - val_accuracy: 0.3721\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.0119 - accuracy: 0.6848 - val_loss: 0.1120 - val_accuracy: 0.3488\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0075 - accuracy: 0.6848 - val_loss: 0.1104 - val_accuracy: 0.3488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbff228d760>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=200, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , ...,  0.00552828,\n",
       "         -0.00659141, -0.00677609],\n",
       "        [ 0.00757559,  0.        ,  0.        , ..., -0.00238965,\n",
       "         -0.01646883, -0.01451381],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00472415,\n",
       "         -0.00212759, -0.00230309],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ...,  0.00232412,\n",
       "          0.00348978,  0.00299009]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.00921513,\n",
       "          0.00537546,  0.00040839],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.0002729 ,\n",
       "          0.00560671,  0.00508392],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00203002,\n",
       "          0.00129858,  0.00159386],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ...,  0.01397219,\n",
       "         -0.00468563, -0.00910651]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.01151   ,\n",
       "         -0.00425536, -0.00547394],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00533579,\n",
       "         -0.00712918, -0.00816971],\n",
       "        [ 0.        ,  0.00757559,  0.        , ...,  0.00495386,\n",
       "          0.00266738,  0.00054649],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00891716,\n",
       "          0.00700444,  0.01336099]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.00381007,\n",
       "          0.00246621,  0.00270697],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00583502,\n",
       "          0.01261146,  0.01024062],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00509299,\n",
       "          0.00385369,  0.00856684],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00514639,\n",
       "          0.00443211,  0.00754019]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.04468748,\n",
       "          0.01935934,  0.00836849],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00039393,\n",
       "         -0.00369049, -0.00365194],\n",
       "        [ 0.        ,  0.00757559,  0.        , ..., -0.00803568,\n",
       "         -0.00708019, -0.00604907],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00954347,\n",
       "          0.00319689,  0.00835033]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.00156536,\n",
       "          0.00570532,  0.00485835],\n",
       "        [ 0.00757559,  0.        ,  0.        , ...,  0.00376246,\n",
       "          0.00060788, -0.00155986],\n",
       "        [ 0.        ,  0.00757559,  0.        , ...,  0.00155012,\n",
       "         -0.00316042, -0.0020672 ],\n",
       "        [ 0.        ,  0.        ,  0.00757559, ..., -0.00264761,\n",
       "         -0.00460923, -0.00221478]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(X)\n",
    "tf.math.l2_normalize(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1249, 57), found shape=(None, 57)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m trainX, testX, trainy, testy \u001b[39m=\u001b[39m  train_test_split(X, Y, test_size \u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m) \n\u001b[0;32m----> 5\u001b[0m evaluate_model(trainX, trainy, testX, testy)\n",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(trainX, trainy, testX, testy)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[39m# fit network\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainy, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     15\u001b[0m \u001b[39m# evaluate model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m _, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(testX, testy, batch_size\u001b[39m=\u001b[39mbatch_size, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/6q/tw4w22c97c98vxdk2crxzdb00000gn/T/__autograph_generated_fileemzwtj1w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/tahaalnufaili/Library/Caches/pypoetry/virtualenvs/harvest-event-detec-Fyp2lGEN-py3.9/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1249, 57), found shape=(None, 57)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy =  train_test_split(X, Y, test_size = 0.4,random_state=42) \n",
    "\n",
    "evaluate_model(trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1249, 1), (834, 57))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape, trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvest-event-detec-Fyp2lGEN-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
